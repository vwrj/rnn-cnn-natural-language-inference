{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for each step in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    print(n, d)\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "def build_vocab():\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    word_vectors = pkl.load(open(\"fasttext_word_vectors.p\", \"rb\"))\n",
    "    id2token = list(word_vectors.keys())\n",
    "    token2id = dict(zip(word_vectors, range(2,2+len(word_vectors)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return word_vectors, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integers(data_label):\n",
    "    for i in range(len(data_label)):\n",
    "        if data_label[i] == \"contradiction\":\n",
    "            data_label[i] = 0\n",
    "        elif data_label[i] == \"entailment\":\n",
    "            data_label[i] = 1\n",
    "        elif data_label[i] == \"neutral\":\n",
    "            data_label[i] = 2\n",
    "    return data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_order(sent1_data, sent2_data, data_label):\n",
    "    i = random.randint(1, len(sent1_data))\n",
    "    print(sent1_data[i])\n",
    "    print(sent2_data[i])\n",
    "    print(data_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize each entry in a list of sentences\n",
    "def tokenize(sentence_list):\n",
    "    return [word_tokenize(sentence_list[i]) for i in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"one-hot encode\": convert each token to id in vocabulary vector (token2id)\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary & embedding matrix from FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights = np.array(list(word_vectors.values()))\n",
    "pad_vec = np.zeros((1, 300))\n",
    "unk_vec = np.random.randn(1, 300) * 0.01\n",
    "pad_unk_vecs = np.vstack((pad_vec, unk_vec))\n",
    "_WEIGHTS = np.vstack((pad_unk_vecs, _weights))\n",
    "_WEIGHTS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pre-process data for TwoSentenceModel\n",
    "#### Shuffle, word tokenize, one-hot index into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(sent1s, sent2s, labels, verify=True):\n",
    "    labels = convert_labels_to_integers(labels)\n",
    "    seed = random.randint(1, 100)\n",
    "    print(\"Random seed for shuffling: {}\".format(seed))\n",
    "    random.Random(seed).shuffle(sent1s)\n",
    "    random.Random(seed).shuffle(sent2s)\n",
    "    random.Random(seed).shuffle(labels)\n",
    "    \n",
    "    print(\"\\nVerifying that the data and label match after shuffling\")\n",
    "    if verify:\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "          \n",
    "    print(\"\\nTokenizing sentence 1 list...\")    \n",
    "    sent1s_tokenized = tokenize(sent1s)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nTokenizing sentence 2 list... \")  \n",
    "    sent2s_tokenized = tokenize(sent2s)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    print(\"\\nOne-hot encoding words for sentence 1 list...\")  \n",
    "    sent1s_indices = token2index_dataset(sent1s_tokenized)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nOne-hot encoding words for sentence 2 list...\")  \n",
    "    sent2s_indices = token2index_dataset(sent2s_tokenized)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return (sent1s_indices, sent2s_indices, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_data_list, sent2_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param sent1_data_list: list of sentence1's (index matches sentence2's and target_list below)\n",
    "        @param sent2_data_list: list of sentence2's\n",
    "        @param target_list: list of correct labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data_list = sent1_data_list\n",
    "        self.sent2_data_list = sent2_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data_list) == len(self.target_list) and len(self.sent2_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[sentence, 1, tokens], [sentence, 2, tokens]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1_tokens_idx = self.sent1_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_tokens_idx = self.sent2_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        combined_tokens_idx = [sent1_tokens_idx, sent2_tokens_idx]\n",
    "        label = self.target_list[key]\n",
    "        return [combined_tokens_idx, len(sent1_tokens_idx), len(sent2_tokens_idx), label]\n",
    "\n",
    "def twosentences_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_length_list = []\n",
    "    label_list = []\n",
    "    combined_data_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[1])\n",
    "        sent2_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0][0]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[0][1]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        combined_data_list.append([padded_vec_1, padded_vec_2])\n",
    "    return [torch.from_numpy(np.array(combined_data_list)), \n",
    "            torch.LongTensor(sent1_length_list), torch.LongTensor(sent2_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 100000\n"
     ]
    }
   ],
   "source": [
    "snli_train = pd.read_csv('snli_train.tsv', sep='\\t')\n",
    "TRAIN_SIZE = 100000\n",
    "\n",
    "sent1_data = list(snli_train[\"sentence1\"])[:TRAIN_SIZE]\n",
    "sent2_data = list(snli_train[\"sentence2\"])[:TRAIN_SIZE]\n",
    "data_label = list(snli_train[\"label\"])[:TRAIN_SIZE]\n",
    "print(\"Size of training data: {}\".format(len(sent1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 91\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "An old man looks at people shopping .\n",
      "An old man is watching people shop .\n",
      "1\n",
      "A brown dog is running along a beach .\n",
      "He is wagging his tail .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Finished creating train_loader.\n"
     ]
    }
   ],
   "source": [
    "sent1_train_indices, sent2_train_indices, train_label = data_pipeline(sent1_data, sent2_data, data_label)\n",
    "train_dataset = TwoSentencesDataset(sent1_train_indices, sent2_train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "print(\"Finished creating train_loader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of val data: 1000\n"
     ]
    }
   ],
   "source": [
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t')\n",
    "sent1_val = list(snli_val[\"sentence1\"])\n",
    "sent2_val = list(snli_val[\"sentence2\"])\n",
    "val_label = list(snli_val[\"label\"])\n",
    "print(\"Size of val data: {}\".format(len(sent1_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 1\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Two men , one with a black shirt and the other with a white shirt , are kicking each other without making contact .\n",
      "Two men are fighting over a girl\n",
      "2\n",
      "Three children run on a sidewalk with a river on one side and a wall on the other .\n",
      "Three children are swimming next to the sidewalk .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sent1_val_indices, sent2_val_indices, val_label = data_pipeline(sent1_val, sent2_val, val_label)\n",
    "val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in snli_train['sentence1']]).describe()['75%']\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoSentenceModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, emb_size = 300):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(TwoSentenceModel, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        weight = torch.FloatTensor(_WEIGHTS)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "#         self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "        # TRYING GRU, UNCOMMENT Below if doing GRU\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        return torch.randn(2, batch_size*2, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        # reset hidden state\n",
    "        batch_size = x.size()[0]\n",
    "                \n",
    "        s1lengths = list(sent1_lengths)\n",
    "        s2lengths = list(sent2_lengths)\n",
    "        ordered_slengths = s1lengths + s2lengths\n",
    "\n",
    "        reverse_sorted_indices = [x for _, x in sorted(zip(ordered_slengths, range(len(ordered_slengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = [x for x, _ in sorted(zip(ordered_slengths, range(len(ordered_slengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = np.array(reverse_sorted_lengths)\n",
    "        \n",
    "        sent1s = x[:, 0, :]\n",
    "        sent2s = x[:, 1, :]\n",
    "        ordered_sents = torch.cat([sent1s, sent2s], dim=0).to(device)\n",
    "        reverse_sorted_data = torch.index_select(ordered_sents, 0, torch.tensor(reverse_sorted_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(reverse_sorted_data)\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, reverse_sorted_lengths, batch_first=True)\n",
    "              \n",
    "        # fprop though RNN\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "                \n",
    "        ### MATCHING BACK\n",
    "        change_it_back = [x for _, x in sorted(zip(reverse_sorted_indices, range(len(reverse_sorted_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "        # 2 by 64 by 250. back in the right order that it came in.        \n",
    "#         hidden_sent1s = self.hidden[0, 0:batch_size, :]\n",
    "#         hidden_sent2s = self.hidden[0, batch_size:, :]\n",
    "              \n",
    "        ### GRU stuff\n",
    "        hidden_sent1s = torch.cat([self.hidden[0, 0:batch_size, :], self.hidden[1, 0:batch_size, :]], dim=1)\n",
    "        hidden_sent2s = torch.cat([self.hidden[0, batch_size:, :], self.hidden[1, batch_size:, :]], dim=1)\n",
    "        \n",
    "#         concatenation of encoded sentences\n",
    "#         linear1 = self.linear1(torch.cat([hidden_sent1s, hidden_sent2s], dim=1))\n",
    "#         addition of encoded sentences\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s) + torch.tensor(hidden_sent2s))\n",
    "#         element-wise multiplication of encoded sentences\n",
    "        linear1 = self.linear1(torch.tensor(hidden_sent1s)*torch.tensor(hidden_sent2s))\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)   \n",
    "#         linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, sent1_lengths, sent2_lengths, labels) in loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.to(device), sent1_lengths.to(device), sent2_lengths.to(device), labels.to(device)\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 7, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    max_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.to(device), sent1_lengths.to(device), sent2_lengths.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, sent1_length_batch, sent2_length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "                \n",
    "    print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/7], Step: [101/3125], Validation Acc: 33.9\n",
      "Epoch: [1/7], Step: [101/3125], Training Loss: 1.107649326324463\n",
      "Epoch: [1/7], Step: [201/3125], Validation Acc: 37.2\n",
      "Epoch: [1/7], Step: [201/3125], Training Loss: 1.080571174621582\n",
      "Epoch: [1/7], Step: [301/3125], Validation Acc: 37.0\n",
      "Epoch: [1/7], Step: [301/3125], Training Loss: 1.116148591041565\n",
      "Epoch: [1/7], Step: [401/3125], Validation Acc: 38.7\n",
      "Epoch: [1/7], Step: [401/3125], Training Loss: 1.0383262634277344\n",
      "Epoch: [1/7], Step: [501/3125], Validation Acc: 37.5\n",
      "Epoch: [1/7], Step: [501/3125], Training Loss: 1.0529433488845825\n",
      "Epoch: [1/7], Step: [601/3125], Validation Acc: 40.6\n",
      "Epoch: [1/7], Step: [601/3125], Training Loss: 1.0518783330917358\n",
      "Epoch: [1/7], Step: [701/3125], Validation Acc: 42.8\n",
      "Epoch: [1/7], Step: [701/3125], Training Loss: 1.1364549398422241\n",
      "Epoch: [1/7], Step: [801/3125], Validation Acc: 42.5\n",
      "Epoch: [1/7], Step: [801/3125], Training Loss: 1.0270627737045288\n",
      "Epoch: [1/7], Step: [901/3125], Validation Acc: 42.9\n",
      "Epoch: [1/7], Step: [901/3125], Training Loss: 1.1183202266693115\n",
      "Epoch: [1/7], Step: [1001/3125], Validation Acc: 44.3\n",
      "Epoch: [1/7], Step: [1001/3125], Training Loss: 1.0649818181991577\n",
      "Epoch: [1/7], Step: [1101/3125], Validation Acc: 43.8\n",
      "Epoch: [1/7], Step: [1101/3125], Training Loss: 1.0730088949203491\n",
      "Epoch: [1/7], Step: [1201/3125], Validation Acc: 48.1\n",
      "Epoch: [1/7], Step: [1201/3125], Training Loss: 1.0909258127212524\n",
      "Epoch: [1/7], Step: [1301/3125], Validation Acc: 49.4\n",
      "Epoch: [1/7], Step: [1301/3125], Training Loss: 1.0468462705612183\n",
      "Epoch: [1/7], Step: [1401/3125], Validation Acc: 48.2\n",
      "Epoch: [1/7], Step: [1401/3125], Training Loss: 0.9959188103675842\n",
      "Epoch: [1/7], Step: [1501/3125], Validation Acc: 49.4\n",
      "Epoch: [1/7], Step: [1501/3125], Training Loss: 0.9785653948783875\n",
      "Epoch: [1/7], Step: [1601/3125], Validation Acc: 50.7\n",
      "Epoch: [1/7], Step: [1601/3125], Training Loss: 1.0751935243606567\n",
      "Epoch: [1/7], Step: [1701/3125], Validation Acc: 52.1\n",
      "Epoch: [1/7], Step: [1701/3125], Training Loss: 0.9590615034103394\n",
      "Epoch: [1/7], Step: [1801/3125], Validation Acc: 52.0\n",
      "Epoch: [1/7], Step: [1801/3125], Training Loss: 0.9857541918754578\n",
      "Epoch: [1/7], Step: [1901/3125], Validation Acc: 49.9\n",
      "Epoch: [1/7], Step: [1901/3125], Training Loss: 0.8838421702384949\n",
      "Epoch: [1/7], Step: [2001/3125], Validation Acc: 53.0\n",
      "Epoch: [1/7], Step: [2001/3125], Training Loss: 0.9635785818099976\n",
      "Epoch: [1/7], Step: [2101/3125], Validation Acc: 54.5\n",
      "Epoch: [1/7], Step: [2101/3125], Training Loss: 0.9852858781814575\n",
      "Epoch: [1/7], Step: [2201/3125], Validation Acc: 55.0\n",
      "Epoch: [1/7], Step: [2201/3125], Training Loss: 0.8012985587120056\n",
      "Epoch: [1/7], Step: [2301/3125], Validation Acc: 54.8\n",
      "Epoch: [1/7], Step: [2301/3125], Training Loss: 0.821895182132721\n",
      "Epoch: [1/7], Step: [2401/3125], Validation Acc: 56.8\n",
      "Epoch: [1/7], Step: [2401/3125], Training Loss: 1.09354829788208\n",
      "Epoch: [1/7], Step: [2501/3125], Validation Acc: 57.6\n",
      "Epoch: [1/7], Step: [2501/3125], Training Loss: 0.8973288536071777\n",
      "Epoch: [1/7], Step: [2601/3125], Validation Acc: 55.5\n",
      "Epoch: [1/7], Step: [2601/3125], Training Loss: 0.942007839679718\n",
      "Epoch: [1/7], Step: [2701/3125], Validation Acc: 55.7\n",
      "Epoch: [1/7], Step: [2701/3125], Training Loss: 0.856865406036377\n",
      "Epoch: [1/7], Step: [2801/3125], Validation Acc: 56.8\n",
      "Epoch: [1/7], Step: [2801/3125], Training Loss: 0.90505051612854\n",
      "Epoch: [1/7], Step: [2901/3125], Validation Acc: 55.9\n",
      "Epoch: [1/7], Step: [2901/3125], Training Loss: 0.9899131655693054\n",
      "Epoch: [1/7], Step: [3001/3125], Validation Acc: 57.3\n",
      "Epoch: [1/7], Step: [3001/3125], Training Loss: 0.9010535478591919\n",
      "Epoch: [1/7], Step: [3101/3125], Validation Acc: 59.0\n",
      "Epoch: [1/7], Step: [3101/3125], Training Loss: 1.1487067937850952\n",
      "Epoch: [2/7], Step: [101/3125], Validation Acc: 57.2\n",
      "Epoch: [2/7], Step: [101/3125], Training Loss: 0.8410359621047974\n",
      "Epoch: [2/7], Step: [201/3125], Validation Acc: 58.9\n",
      "Epoch: [2/7], Step: [201/3125], Training Loss: 0.795361340045929\n",
      "Epoch: [2/7], Step: [301/3125], Validation Acc: 58.6\n",
      "Epoch: [2/7], Step: [301/3125], Training Loss: 1.016220211982727\n",
      "Epoch: [2/7], Step: [401/3125], Validation Acc: 55.4\n",
      "Epoch: [2/7], Step: [401/3125], Training Loss: 0.7851451635360718\n",
      "Epoch: [2/7], Step: [501/3125], Validation Acc: 57.4\n",
      "Epoch: [2/7], Step: [501/3125], Training Loss: 0.9423165917396545\n",
      "Epoch: [2/7], Step: [601/3125], Validation Acc: 61.7\n",
      "Epoch: [2/7], Step: [601/3125], Training Loss: 0.8101382851600647\n",
      "Epoch: [2/7], Step: [701/3125], Validation Acc: 59.3\n",
      "Epoch: [2/7], Step: [701/3125], Training Loss: 0.968680739402771\n",
      "Epoch: [2/7], Step: [801/3125], Validation Acc: 59.0\n",
      "Epoch: [2/7], Step: [801/3125], Training Loss: 0.838695228099823\n",
      "Epoch: [2/7], Step: [901/3125], Validation Acc: 58.5\n",
      "Epoch: [2/7], Step: [901/3125], Training Loss: 0.7860754132270813\n",
      "Epoch: [2/7], Step: [1001/3125], Validation Acc: 59.1\n",
      "Epoch: [2/7], Step: [1001/3125], Training Loss: 0.7936023473739624\n",
      "Epoch: [2/7], Step: [1101/3125], Validation Acc: 59.6\n",
      "Epoch: [2/7], Step: [1101/3125], Training Loss: 1.0289722681045532\n",
      "Epoch: [2/7], Step: [1201/3125], Validation Acc: 59.8\n",
      "Epoch: [2/7], Step: [1201/3125], Training Loss: 0.9198770523071289\n",
      "Epoch: [2/7], Step: [1301/3125], Validation Acc: 60.0\n",
      "Epoch: [2/7], Step: [1301/3125], Training Loss: 0.8510566353797913\n",
      "Epoch: [2/7], Step: [1401/3125], Validation Acc: 59.2\n",
      "Epoch: [2/7], Step: [1401/3125], Training Loss: 0.7813944816589355\n",
      "Epoch: [2/7], Step: [1501/3125], Validation Acc: 60.8\n",
      "Epoch: [2/7], Step: [1501/3125], Training Loss: 0.8144502639770508\n",
      "Epoch: [2/7], Step: [1601/3125], Validation Acc: 61.2\n",
      "Epoch: [2/7], Step: [1601/3125], Training Loss: 0.8966240882873535\n",
      "Epoch: [2/7], Step: [1701/3125], Validation Acc: 61.6\n",
      "Epoch: [2/7], Step: [1701/3125], Training Loss: 0.8266281485557556\n",
      "Epoch: [2/7], Step: [1801/3125], Validation Acc: 61.1\n",
      "Epoch: [2/7], Step: [1801/3125], Training Loss: 0.9702677726745605\n",
      "Epoch: [2/7], Step: [1901/3125], Validation Acc: 58.7\n",
      "Epoch: [2/7], Step: [1901/3125], Training Loss: 0.7823871374130249\n",
      "Epoch: [2/7], Step: [2001/3125], Validation Acc: 59.8\n",
      "Epoch: [2/7], Step: [2001/3125], Training Loss: 0.7384819388389587\n",
      "Epoch: [2/7], Step: [2101/3125], Validation Acc: 63.1\n",
      "Epoch: [2/7], Step: [2101/3125], Training Loss: 0.8020042181015015\n",
      "Epoch: [2/7], Step: [2201/3125], Validation Acc: 61.3\n",
      "Epoch: [2/7], Step: [2201/3125], Training Loss: 0.7282341122627258\n",
      "Epoch: [2/7], Step: [2301/3125], Validation Acc: 60.8\n",
      "Epoch: [2/7], Step: [2301/3125], Training Loss: 0.7375141382217407\n",
      "Epoch: [2/7], Step: [2401/3125], Validation Acc: 63.7\n",
      "Epoch: [2/7], Step: [2401/3125], Training Loss: 1.1328892707824707\n",
      "Epoch: [2/7], Step: [2501/3125], Validation Acc: 62.2\n",
      "Epoch: [2/7], Step: [2501/3125], Training Loss: 0.8410861492156982\n",
      "Epoch: [2/7], Step: [2601/3125], Validation Acc: 61.2\n",
      "Epoch: [2/7], Step: [2601/3125], Training Loss: 0.8495203256607056\n",
      "Epoch: [2/7], Step: [2701/3125], Validation Acc: 61.6\n",
      "Epoch: [2/7], Step: [2701/3125], Training Loss: 0.7896111011505127\n",
      "Epoch: [2/7], Step: [2801/3125], Validation Acc: 62.5\n",
      "Epoch: [2/7], Step: [2801/3125], Training Loss: 0.9678950309753418\n",
      "Epoch: [2/7], Step: [2901/3125], Validation Acc: 59.4\n",
      "Epoch: [2/7], Step: [2901/3125], Training Loss: 0.6593627333641052\n",
      "Epoch: [2/7], Step: [3001/3125], Validation Acc: 61.3\n",
      "Epoch: [2/7], Step: [3001/3125], Training Loss: 0.6650041937828064\n",
      "Epoch: [2/7], Step: [3101/3125], Validation Acc: 61.4\n",
      "Epoch: [2/7], Step: [3101/3125], Training Loss: 0.9553378224372864\n",
      "Epoch: [3/7], Step: [101/3125], Validation Acc: 62.3\n",
      "Epoch: [3/7], Step: [101/3125], Training Loss: 0.7743453979492188\n",
      "Epoch: [3/7], Step: [201/3125], Validation Acc: 63.2\n",
      "Epoch: [3/7], Step: [201/3125], Training Loss: 0.712113082408905\n",
      "Epoch: [3/7], Step: [301/3125], Validation Acc: 62.1\n",
      "Epoch: [3/7], Step: [301/3125], Training Loss: 1.0047738552093506\n",
      "Epoch: [3/7], Step: [401/3125], Validation Acc: 61.7\n",
      "Epoch: [3/7], Step: [401/3125], Training Loss: 0.6689044833183289\n",
      "Epoch: [3/7], Step: [501/3125], Validation Acc: 62.2\n",
      "Epoch: [3/7], Step: [501/3125], Training Loss: 0.7326820492744446\n",
      "Epoch: [3/7], Step: [601/3125], Validation Acc: 65.1\n",
      "Epoch: [3/7], Step: [601/3125], Training Loss: 0.7256308794021606\n",
      "Epoch: [3/7], Step: [701/3125], Validation Acc: 63.2\n",
      "Epoch: [3/7], Step: [701/3125], Training Loss: 0.8444808125495911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/7], Step: [801/3125], Validation Acc: 62.9\n",
      "Epoch: [3/7], Step: [801/3125], Training Loss: 0.7256885766983032\n",
      "Epoch: [3/7], Step: [901/3125], Validation Acc: 64.0\n",
      "Epoch: [3/7], Step: [901/3125], Training Loss: 0.7802861332893372\n",
      "Epoch: [3/7], Step: [1001/3125], Validation Acc: 64.2\n",
      "Epoch: [3/7], Step: [1001/3125], Training Loss: 0.7330567240715027\n",
      "Epoch: [3/7], Step: [1101/3125], Validation Acc: 62.7\n",
      "Epoch: [3/7], Step: [1101/3125], Training Loss: 0.9180753231048584\n",
      "Epoch: [3/7], Step: [1201/3125], Validation Acc: 63.1\n",
      "Epoch: [3/7], Step: [1201/3125], Training Loss: 0.7830043435096741\n",
      "Epoch: [3/7], Step: [1301/3125], Validation Acc: 64.2\n",
      "Epoch: [3/7], Step: [1301/3125], Training Loss: 0.7513207197189331\n",
      "Epoch: [3/7], Step: [1401/3125], Validation Acc: 64.6\n",
      "Epoch: [3/7], Step: [1401/3125], Training Loss: 0.7049599885940552\n",
      "Epoch: [3/7], Step: [1501/3125], Validation Acc: 64.7\n",
      "Epoch: [3/7], Step: [1501/3125], Training Loss: 0.7262147068977356\n",
      "Epoch: [3/7], Step: [1601/3125], Validation Acc: 64.5\n",
      "Epoch: [3/7], Step: [1601/3125], Training Loss: 0.7515585422515869\n",
      "Epoch: [3/7], Step: [1701/3125], Validation Acc: 64.3\n",
      "Epoch: [3/7], Step: [1701/3125], Training Loss: 0.6391506791114807\n",
      "Epoch: [3/7], Step: [1801/3125], Validation Acc: 64.2\n",
      "Epoch: [3/7], Step: [1801/3125], Training Loss: 0.8613108992576599\n",
      "Epoch: [3/7], Step: [1901/3125], Validation Acc: 61.9\n",
      "Epoch: [3/7], Step: [1901/3125], Training Loss: 0.7102853059768677\n",
      "Epoch: [3/7], Step: [2001/3125], Validation Acc: 63.5\n",
      "Epoch: [3/7], Step: [2001/3125], Training Loss: 0.6928712129592896\n",
      "Epoch: [3/7], Step: [2101/3125], Validation Acc: 65.5\n",
      "Epoch: [3/7], Step: [2101/3125], Training Loss: 0.7509967088699341\n",
      "Epoch: [3/7], Step: [2201/3125], Validation Acc: 64.5\n",
      "Epoch: [3/7], Step: [2201/3125], Training Loss: 0.6375235319137573\n",
      "Epoch: [3/7], Step: [2301/3125], Validation Acc: 65.3\n",
      "Epoch: [3/7], Step: [2301/3125], Training Loss: 0.7426446080207825\n",
      "Epoch: [3/7], Step: [2401/3125], Validation Acc: 65.9\n",
      "Epoch: [3/7], Step: [2401/3125], Training Loss: 1.045562982559204\n",
      "Epoch: [3/7], Step: [2501/3125], Validation Acc: 64.4\n",
      "Epoch: [3/7], Step: [2501/3125], Training Loss: 0.7017817497253418\n",
      "Epoch: [3/7], Step: [2601/3125], Validation Acc: 65.0\n",
      "Epoch: [3/7], Step: [2601/3125], Training Loss: 0.7158330678939819\n",
      "Epoch: [3/7], Step: [2701/3125], Validation Acc: 65.9\n",
      "Epoch: [3/7], Step: [2701/3125], Training Loss: 0.7115272879600525\n",
      "Epoch: [3/7], Step: [2801/3125], Validation Acc: 66.6\n",
      "Epoch: [3/7], Step: [2801/3125], Training Loss: 1.1439077854156494\n",
      "Epoch: [3/7], Step: [2901/3125], Validation Acc: 63.8\n",
      "Epoch: [3/7], Step: [2901/3125], Training Loss: 0.5495975613594055\n",
      "Epoch: [3/7], Step: [3001/3125], Validation Acc: 65.7\n",
      "Epoch: [3/7], Step: [3001/3125], Training Loss: 0.4991990923881531\n",
      "Epoch: [3/7], Step: [3101/3125], Validation Acc: 65.4\n",
      "Epoch: [3/7], Step: [3101/3125], Training Loss: 0.823597252368927\n",
      "Epoch: [4/7], Step: [101/3125], Validation Acc: 64.5\n",
      "Epoch: [4/7], Step: [101/3125], Training Loss: 0.7182996869087219\n",
      "Epoch: [4/7], Step: [201/3125], Validation Acc: 67.0\n",
      "Epoch: [4/7], Step: [201/3125], Training Loss: 0.5837921500205994\n",
      "Epoch: [4/7], Step: [301/3125], Validation Acc: 67.2\n",
      "Epoch: [4/7], Step: [301/3125], Training Loss: 1.0125452280044556\n",
      "Epoch: [4/7], Step: [401/3125], Validation Acc: 65.7\n",
      "Epoch: [4/7], Step: [401/3125], Training Loss: 0.6038736701011658\n",
      "Epoch: [4/7], Step: [501/3125], Validation Acc: 65.9\n",
      "Epoch: [4/7], Step: [501/3125], Training Loss: 0.6316654682159424\n",
      "Epoch: [4/7], Step: [601/3125], Validation Acc: 67.5\n",
      "Epoch: [4/7], Step: [601/3125], Training Loss: 0.6642276644706726\n",
      "Epoch: [4/7], Step: [701/3125], Validation Acc: 66.5\n",
      "Epoch: [4/7], Step: [701/3125], Training Loss: 0.7709907293319702\n",
      "Epoch: [4/7], Step: [801/3125], Validation Acc: 65.8\n",
      "Epoch: [4/7], Step: [801/3125], Training Loss: 0.7865604162216187\n",
      "Epoch: [4/7], Step: [901/3125], Validation Acc: 66.1\n",
      "Epoch: [4/7], Step: [901/3125], Training Loss: 0.7632544040679932\n",
      "Epoch: [4/7], Step: [1001/3125], Validation Acc: 66.6\n",
      "Epoch: [4/7], Step: [1001/3125], Training Loss: 0.6559262871742249\n",
      "Epoch: [4/7], Step: [1101/3125], Validation Acc: 65.2\n",
      "Epoch: [4/7], Step: [1101/3125], Training Loss: 0.8451694846153259\n",
      "Epoch: [4/7], Step: [1201/3125], Validation Acc: 65.8\n",
      "Epoch: [4/7], Step: [1201/3125], Training Loss: 0.6922621130943298\n",
      "Epoch: [4/7], Step: [1301/3125], Validation Acc: 67.7\n",
      "Epoch: [4/7], Step: [1301/3125], Training Loss: 0.780856728553772\n",
      "Epoch: [4/7], Step: [1401/3125], Validation Acc: 66.6\n",
      "Epoch: [4/7], Step: [1401/3125], Training Loss: 0.6833751797676086\n",
      "Epoch: [4/7], Step: [1501/3125], Validation Acc: 65.3\n",
      "Epoch: [4/7], Step: [1501/3125], Training Loss: 0.7442052364349365\n",
      "Epoch: [4/7], Step: [1601/3125], Validation Acc: 65.8\n",
      "Epoch: [4/7], Step: [1601/3125], Training Loss: 0.5989984273910522\n",
      "Epoch: [4/7], Step: [1701/3125], Validation Acc: 65.6\n",
      "Epoch: [4/7], Step: [1701/3125], Training Loss: 0.5856050252914429\n",
      "Epoch: [4/7], Step: [1801/3125], Validation Acc: 66.1\n",
      "Epoch: [4/7], Step: [1801/3125], Training Loss: 0.7600874304771423\n",
      "Epoch: [4/7], Step: [1901/3125], Validation Acc: 64.5\n",
      "Epoch: [4/7], Step: [1901/3125], Training Loss: 0.650307297706604\n",
      "Epoch: [4/7], Step: [2001/3125], Validation Acc: 66.4\n",
      "Epoch: [4/7], Step: [2001/3125], Training Loss: 0.6728202700614929\n",
      "Epoch: [4/7], Step: [2101/3125], Validation Acc: 68.0\n",
      "Epoch: [4/7], Step: [2101/3125], Training Loss: 0.6802610158920288\n",
      "Epoch: [4/7], Step: [2201/3125], Validation Acc: 67.4\n",
      "Epoch: [4/7], Step: [2201/3125], Training Loss: 0.6015694737434387\n",
      "Epoch: [4/7], Step: [2301/3125], Validation Acc: 68.0\n",
      "Epoch: [4/7], Step: [2301/3125], Training Loss: 0.7267804741859436\n",
      "Epoch: [4/7], Step: [2401/3125], Validation Acc: 67.6\n",
      "Epoch: [4/7], Step: [2401/3125], Training Loss: 0.842971920967102\n",
      "Epoch: [4/7], Step: [2501/3125], Validation Acc: 67.6\n",
      "Epoch: [4/7], Step: [2501/3125], Training Loss: 0.6368414163589478\n",
      "Epoch: [4/7], Step: [2601/3125], Validation Acc: 67.0\n",
      "Epoch: [4/7], Step: [2601/3125], Training Loss: 0.6588208079338074\n",
      "Epoch: [4/7], Step: [2701/3125], Validation Acc: 68.1\n",
      "Epoch: [4/7], Step: [2701/3125], Training Loss: 0.6607432961463928\n",
      "Epoch: [4/7], Step: [2801/3125], Validation Acc: 68.7\n",
      "Epoch: [4/7], Step: [2801/3125], Training Loss: 1.0588711500167847\n",
      "Epoch: [4/7], Step: [2901/3125], Validation Acc: 65.2\n",
      "Epoch: [4/7], Step: [2901/3125], Training Loss: 0.4848821759223938\n",
      "Epoch: [4/7], Step: [3001/3125], Validation Acc: 66.2\n",
      "Epoch: [4/7], Step: [3001/3125], Training Loss: 0.4551945924758911\n",
      "Epoch: [4/7], Step: [3101/3125], Validation Acc: 66.7\n",
      "Epoch: [4/7], Step: [3101/3125], Training Loss: 0.7943025827407837\n",
      "Epoch: [5/7], Step: [101/3125], Validation Acc: 67.0\n",
      "Epoch: [5/7], Step: [101/3125], Training Loss: 0.6709041595458984\n",
      "Epoch: [5/7], Step: [201/3125], Validation Acc: 67.0\n",
      "Epoch: [5/7], Step: [201/3125], Training Loss: 0.4837452471256256\n",
      "Epoch: [5/7], Step: [301/3125], Validation Acc: 67.0\n",
      "Epoch: [5/7], Step: [301/3125], Training Loss: 0.9911916255950928\n",
      "Epoch: [5/7], Step: [401/3125], Validation Acc: 65.1\n",
      "Epoch: [5/7], Step: [401/3125], Training Loss: 0.6007481813430786\n",
      "Epoch: [5/7], Step: [501/3125], Validation Acc: 66.4\n",
      "Epoch: [5/7], Step: [501/3125], Training Loss: 0.5118472576141357\n",
      "Epoch: [5/7], Step: [601/3125], Validation Acc: 67.5\n",
      "Epoch: [5/7], Step: [601/3125], Training Loss: 0.5744597911834717\n",
      "Epoch: [5/7], Step: [701/3125], Validation Acc: 66.9\n",
      "Epoch: [5/7], Step: [701/3125], Training Loss: 0.806390106678009\n",
      "Epoch: [5/7], Step: [801/3125], Validation Acc: 67.5\n",
      "Epoch: [5/7], Step: [801/3125], Training Loss: 0.7510694861412048\n",
      "Epoch: [5/7], Step: [901/3125], Validation Acc: 68.9\n",
      "Epoch: [5/7], Step: [901/3125], Training Loss: 0.6726533770561218\n",
      "Epoch: [5/7], Step: [1001/3125], Validation Acc: 67.9\n",
      "Epoch: [5/7], Step: [1001/3125], Training Loss: 0.6452460289001465\n",
      "Epoch: [5/7], Step: [1101/3125], Validation Acc: 67.4\n",
      "Epoch: [5/7], Step: [1101/3125], Training Loss: 0.8189246654510498\n",
      "Epoch: [5/7], Step: [1201/3125], Validation Acc: 66.9\n",
      "Epoch: [5/7], Step: [1201/3125], Training Loss: 0.5854706764221191\n",
      "Epoch: [5/7], Step: [1301/3125], Validation Acc: 67.0\n",
      "Epoch: [5/7], Step: [1301/3125], Training Loss: 0.7274656891822815\n",
      "Epoch: [5/7], Step: [1401/3125], Validation Acc: 68.4\n",
      "Epoch: [5/7], Step: [1401/3125], Training Loss: 0.6639606952667236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/7], Step: [1501/3125], Validation Acc: 68.7\n",
      "Epoch: [5/7], Step: [1501/3125], Training Loss: 0.6363848447799683\n",
      "Epoch: [5/7], Step: [1601/3125], Validation Acc: 65.9\n",
      "Epoch: [5/7], Step: [1601/3125], Training Loss: 0.5829861760139465\n",
      "Epoch: [5/7], Step: [1701/3125], Validation Acc: 67.7\n",
      "Epoch: [5/7], Step: [1701/3125], Training Loss: 0.5647152662277222\n",
      "Epoch: [5/7], Step: [1801/3125], Validation Acc: 66.9\n",
      "Epoch: [5/7], Step: [1801/3125], Training Loss: 0.6764464974403381\n",
      "Epoch: [5/7], Step: [1901/3125], Validation Acc: 65.8\n",
      "Epoch: [5/7], Step: [1901/3125], Training Loss: 0.6128199100494385\n",
      "Epoch: [5/7], Step: [2001/3125], Validation Acc: 66.9\n",
      "Epoch: [5/7], Step: [2001/3125], Training Loss: 0.6860468983650208\n",
      "Epoch: [5/7], Step: [2101/3125], Validation Acc: 68.2\n",
      "Epoch: [5/7], Step: [2101/3125], Training Loss: 0.6453467607498169\n",
      "Epoch: [5/7], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [5/7], Step: [2201/3125], Training Loss: 0.5940616130828857\n",
      "Epoch: [5/7], Step: [2301/3125], Validation Acc: 68.6\n",
      "Epoch: [5/7], Step: [2301/3125], Training Loss: 0.6898934841156006\n",
      "Epoch: [5/7], Step: [2401/3125], Validation Acc: 68.9\n",
      "Epoch: [5/7], Step: [2401/3125], Training Loss: 0.8697193264961243\n",
      "Epoch: [5/7], Step: [2501/3125], Validation Acc: 67.0\n",
      "Epoch: [5/7], Step: [2501/3125], Training Loss: 0.6114788055419922\n",
      "Epoch: [5/7], Step: [2601/3125], Validation Acc: 67.3\n",
      "Epoch: [5/7], Step: [2601/3125], Training Loss: 0.672959566116333\n",
      "Epoch: [5/7], Step: [2701/3125], Validation Acc: 67.4\n",
      "Epoch: [5/7], Step: [2701/3125], Training Loss: 0.667005717754364\n",
      "Epoch: [5/7], Step: [2801/3125], Validation Acc: 68.8\n",
      "Epoch: [5/7], Step: [2801/3125], Training Loss: 1.0021780729293823\n",
      "Epoch: [5/7], Step: [2901/3125], Validation Acc: 66.5\n",
      "Epoch: [5/7], Step: [2901/3125], Training Loss: 0.4365755319595337\n",
      "Epoch: [5/7], Step: [3001/3125], Validation Acc: 67.4\n",
      "Epoch: [5/7], Step: [3001/3125], Training Loss: 0.43514716625213623\n",
      "Epoch: [5/7], Step: [3101/3125], Validation Acc: 68.3\n",
      "Epoch: [5/7], Step: [3101/3125], Training Loss: 0.7216150164604187\n",
      "Epoch: [6/7], Step: [101/3125], Validation Acc: 66.6\n",
      "Epoch: [6/7], Step: [101/3125], Training Loss: 0.704730749130249\n",
      "Epoch: [6/7], Step: [201/3125], Validation Acc: 68.1\n",
      "Epoch: [6/7], Step: [201/3125], Training Loss: 0.41888749599456787\n",
      "Epoch: [6/7], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [6/7], Step: [301/3125], Training Loss: 0.9059993028640747\n",
      "Epoch: [6/7], Step: [401/3125], Validation Acc: 67.1\n",
      "Epoch: [6/7], Step: [401/3125], Training Loss: 0.573148250579834\n",
      "Epoch: [6/7], Step: [501/3125], Validation Acc: 66.8\n",
      "Epoch: [6/7], Step: [501/3125], Training Loss: 0.48116615414619446\n",
      "Epoch: [6/7], Step: [601/3125], Validation Acc: 68.3\n",
      "Epoch: [6/7], Step: [601/3125], Training Loss: 0.5377804040908813\n",
      "Epoch: [6/7], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/7], Step: [701/3125], Training Loss: 0.7431805729866028\n",
      "Epoch: [6/7], Step: [801/3125], Validation Acc: 67.8\n",
      "Epoch: [6/7], Step: [801/3125], Training Loss: 0.694939911365509\n",
      "Epoch: [6/7], Step: [901/3125], Validation Acc: 69.4\n",
      "Epoch: [6/7], Step: [901/3125], Training Loss: 0.5938137173652649\n",
      "Epoch: [6/7], Step: [1001/3125], Validation Acc: 68.5\n",
      "Epoch: [6/7], Step: [1001/3125], Training Loss: 0.525096595287323\n",
      "Epoch: [6/7], Step: [1101/3125], Validation Acc: 69.1\n",
      "Epoch: [6/7], Step: [1101/3125], Training Loss: 0.7465308308601379\n",
      "Epoch: [6/7], Step: [1201/3125], Validation Acc: 67.7\n",
      "Epoch: [6/7], Step: [1201/3125], Training Loss: 0.5348671078681946\n",
      "Epoch: [6/7], Step: [1301/3125], Validation Acc: 68.7\n",
      "Epoch: [6/7], Step: [1301/3125], Training Loss: 0.6362058520317078\n",
      "Epoch: [6/7], Step: [1401/3125], Validation Acc: 69.1\n",
      "Epoch: [6/7], Step: [1401/3125], Training Loss: 0.5806451439857483\n",
      "Epoch: [6/7], Step: [1501/3125], Validation Acc: 69.0\n",
      "Epoch: [6/7], Step: [1501/3125], Training Loss: 0.6023889780044556\n",
      "Epoch: [6/7], Step: [1601/3125], Validation Acc: 65.9\n",
      "Epoch: [6/7], Step: [1601/3125], Training Loss: 0.5405868887901306\n",
      "Epoch: [6/7], Step: [1701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/7], Step: [1701/3125], Training Loss: 0.5502942800521851\n",
      "Epoch: [6/7], Step: [1801/3125], Validation Acc: 67.3\n",
      "Epoch: [6/7], Step: [1801/3125], Training Loss: 0.6099789142608643\n",
      "Epoch: [6/7], Step: [1901/3125], Validation Acc: 66.0\n",
      "Epoch: [6/7], Step: [1901/3125], Training Loss: 0.5572255253791809\n",
      "Epoch: [6/7], Step: [2001/3125], Validation Acc: 66.7\n",
      "Epoch: [6/7], Step: [2001/3125], Training Loss: 0.6526479125022888\n",
      "Epoch: [6/7], Step: [2101/3125], Validation Acc: 68.8\n",
      "Epoch: [6/7], Step: [2101/3125], Training Loss: 0.5697402358055115\n",
      "Epoch: [6/7], Step: [2201/3125], Validation Acc: 68.7\n",
      "Epoch: [6/7], Step: [2201/3125], Training Loss: 0.5655016303062439\n",
      "Epoch: [6/7], Step: [2301/3125], Validation Acc: 68.4\n",
      "Epoch: [6/7], Step: [2301/3125], Training Loss: 0.6241071820259094\n",
      "Epoch: [6/7], Step: [2401/3125], Validation Acc: 68.8\n",
      "Epoch: [6/7], Step: [2401/3125], Training Loss: 0.7433909177780151\n",
      "Epoch: [6/7], Step: [2501/3125], Validation Acc: 68.2\n",
      "Epoch: [6/7], Step: [2501/3125], Training Loss: 0.5068173408508301\n",
      "Epoch: [6/7], Step: [2601/3125], Validation Acc: 67.7\n",
      "Epoch: [6/7], Step: [2601/3125], Training Loss: 0.6759504079818726\n",
      "Epoch: [6/7], Step: [2701/3125], Validation Acc: 68.9\n",
      "Epoch: [6/7], Step: [2701/3125], Training Loss: 0.6785545349121094\n",
      "Epoch: [6/7], Step: [2801/3125], Validation Acc: 69.6\n",
      "Epoch: [6/7], Step: [2801/3125], Training Loss: 0.908778190612793\n",
      "Epoch: [6/7], Step: [2901/3125], Validation Acc: 68.4\n",
      "Epoch: [6/7], Step: [2901/3125], Training Loss: 0.43013450503349304\n",
      "Epoch: [6/7], Step: [3001/3125], Validation Acc: 69.0\n",
      "Epoch: [6/7], Step: [3001/3125], Training Loss: 0.40313389897346497\n",
      "Epoch: [6/7], Step: [3101/3125], Validation Acc: 69.2\n",
      "Epoch: [6/7], Step: [3101/3125], Training Loss: 0.5971359610557556\n",
      "Epoch: [7/7], Step: [101/3125], Validation Acc: 67.6\n",
      "Epoch: [7/7], Step: [101/3125], Training Loss: 0.6730759143829346\n",
      "Epoch: [7/7], Step: [201/3125], Validation Acc: 68.9\n",
      "Epoch: [7/7], Step: [201/3125], Training Loss: 0.4056824743747711\n",
      "Epoch: [7/7], Step: [301/3125], Validation Acc: 68.2\n",
      "Epoch: [7/7], Step: [301/3125], Training Loss: 0.8421414494514465\n",
      "Epoch: [7/7], Step: [401/3125], Validation Acc: 66.7\n",
      "Epoch: [7/7], Step: [401/3125], Training Loss: 0.506800651550293\n",
      "Epoch: [7/7], Step: [501/3125], Validation Acc: 67.6\n",
      "Epoch: [7/7], Step: [501/3125], Training Loss: 0.41356831789016724\n",
      "Epoch: [7/7], Step: [601/3125], Validation Acc: 69.4\n",
      "Epoch: [7/7], Step: [601/3125], Training Loss: 0.5243743658065796\n",
      "Epoch: [7/7], Step: [701/3125], Validation Acc: 66.8\n",
      "Epoch: [7/7], Step: [701/3125], Training Loss: 0.6939132213592529\n",
      "Epoch: [7/7], Step: [801/3125], Validation Acc: 68.2\n",
      "Epoch: [7/7], Step: [801/3125], Training Loss: 0.6282557845115662\n",
      "Epoch: [7/7], Step: [901/3125], Validation Acc: 69.5\n",
      "Epoch: [7/7], Step: [901/3125], Training Loss: 0.6413891911506653\n",
      "Epoch: [7/7], Step: [1001/3125], Validation Acc: 67.9\n",
      "Epoch: [7/7], Step: [1001/3125], Training Loss: 0.4751178026199341\n",
      "Epoch: [7/7], Step: [1101/3125], Validation Acc: 70.0\n",
      "Epoch: [7/7], Step: [1101/3125], Training Loss: 0.6895475387573242\n",
      "Epoch: [7/7], Step: [1201/3125], Validation Acc: 68.7\n",
      "Epoch: [7/7], Step: [1201/3125], Training Loss: 0.49119195342063904\n",
      "Epoch: [7/7], Step: [1301/3125], Validation Acc: 70.2\n",
      "Epoch: [7/7], Step: [1301/3125], Training Loss: 0.5852890014648438\n",
      "Epoch: [7/7], Step: [1401/3125], Validation Acc: 69.5\n",
      "Epoch: [7/7], Step: [1401/3125], Training Loss: 0.5255852341651917\n",
      "Epoch: [7/7], Step: [1501/3125], Validation Acc: 68.2\n",
      "Epoch: [7/7], Step: [1501/3125], Training Loss: 0.5769892334938049\n",
      "Epoch: [7/7], Step: [1601/3125], Validation Acc: 66.9\n",
      "Epoch: [7/7], Step: [1601/3125], Training Loss: 0.49708423018455505\n",
      "Epoch: [7/7], Step: [1701/3125], Validation Acc: 67.5\n",
      "Epoch: [7/7], Step: [1701/3125], Training Loss: 0.5224326848983765\n",
      "Epoch: [7/7], Step: [1801/3125], Validation Acc: 68.1\n",
      "Epoch: [7/7], Step: [1801/3125], Training Loss: 0.5134520530700684\n",
      "Epoch: [7/7], Step: [1901/3125], Validation Acc: 68.0\n",
      "Epoch: [7/7], Step: [1901/3125], Training Loss: 0.495788037776947\n",
      "Epoch: [7/7], Step: [2001/3125], Validation Acc: 68.0\n",
      "Epoch: [7/7], Step: [2001/3125], Training Loss: 0.574986457824707\n",
      "Epoch: [7/7], Step: [2101/3125], Validation Acc: 68.8\n",
      "Epoch: [7/7], Step: [2101/3125], Training Loss: 0.5021686553955078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/7], Step: [2201/3125], Validation Acc: 68.8\n",
      "Epoch: [7/7], Step: [2201/3125], Training Loss: 0.5369063019752502\n",
      "Epoch: [7/7], Step: [2301/3125], Validation Acc: 70.0\n",
      "Epoch: [7/7], Step: [2301/3125], Training Loss: 0.5517008304595947\n",
      "Epoch: [7/7], Step: [2401/3125], Validation Acc: 68.0\n",
      "Epoch: [7/7], Step: [2401/3125], Training Loss: 0.6681950092315674\n",
      "Epoch: [7/7], Step: [2501/3125], Validation Acc: 68.5\n",
      "Epoch: [7/7], Step: [2501/3125], Training Loss: 0.5436170697212219\n",
      "Epoch: [7/7], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [7/7], Step: [2601/3125], Training Loss: 0.5800732970237732\n",
      "Epoch: [7/7], Step: [2701/3125], Validation Acc: 68.1\n",
      "Epoch: [7/7], Step: [2701/3125], Training Loss: 0.5750572085380554\n",
      "Epoch: [7/7], Step: [2801/3125], Validation Acc: 68.4\n",
      "Epoch: [7/7], Step: [2801/3125], Training Loss: 0.7890423536300659\n",
      "Epoch: [7/7], Step: [2901/3125], Validation Acc: 67.0\n",
      "Epoch: [7/7], Step: [2901/3125], Training Loss: 0.38154488801956177\n",
      "Epoch: [7/7], Step: [3001/3125], Validation Acc: 69.1\n",
      "Epoch: [7/7], Step: [3001/3125], Training Loss: 0.4064396023750305\n",
      "Epoch: [7/7], Step: [3101/3125], Validation Acc: 68.9\n",
      "Epoch: [7/7], Step: [3101/3125], Training Loss: 0.5125353932380676\n",
      "Max Validation Accuracy: 70.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70.2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwoSentenceModel(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).to(device)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/home/vrajiv/rnn-cnn-natural-language-inference/best_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val = pd.read_csv('mnli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile for sentence length (in characters): 151.0\n"
     ]
    }
   ],
   "source": [
    "sentence_length_75 = pd.Series([len(x) for x in mnli_val['sentence1']]).describe()['75%']\n",
    "print(\"75th percentile for sentence length (in characters): {}\".format(sentence_length_75))\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary of (sent1, sent2, label) data, by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_dict = {}\n",
    "for x in mnli_val['genre'].unique():\n",
    "    filtered = mnli_val[mnli_val['genre'] == x]\n",
    "    mnli_val_dict[x] = {}\n",
    "    mnli_val_dict[x][\"sent1s\"] = list(filtered[\"sentence1\"])\n",
    "    mnli_val_dict[x][\"sent2s\"] = list(filtered[\"sentence2\"])\n",
    "    mnli_val_dict[x][\"label\"] = convert_labels_to_integers(list(filtered[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "[0 1 2]\n",
      "1005\n",
      "[0 1 2]\n",
      "1002\n",
      "[0 1 2]\n",
      "1016\n",
      "[0 1 2]\n",
      "982\n",
      "[0 1 2]\n",
      "Now open political debate flourished , especially in Calcutta where Karl Marx was much appreciated .\n",
      "Now political debate died down in Calcutta especially , where Karl Marx was hated .\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for x in mnli_val_dict.keys():\n",
    "    print(len(mnli_val_dict[x][\"sent1s\"]))\n",
    "    print(np.unique(mnli_val_dict[x][\"label\"]))\n",
    "    \n",
    "# quick verify\n",
    "verify_order(mnli_val_dict['travel'][\"sent1s\"], mnli_val_dict['travel'][\"sent2s\"], mnli_val_dict['travel'][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 95\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "'You burned down my house . '\n",
      "'You used matches and gasoline to commit arson . '\n",
      "2\n",
      "I sha n't stop you . ''\n",
      "You can stop .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 79\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "maybe adult literacy maybe you know composition writing maybe you know uh volunteering you know on a tutor line or though the even through the elementary schools for help with homework or the other part of me says is God i 've had enough kids do i really\n",
      "maybe I could volunteer to help coach sports since I 've helped all my children be successful in sports\n",
      "0\n",
      "that 's right you can work yourself to death well i 'm sorry to hear your color did n't come out so good over the weekend\n",
      "I 'm sorry it did n't turn out as planned .\n",
      "1\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 51\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "He 's too cautious .\n",
      "He is cautious due to a lack of confidence .\n",
      "2\n",
      "Acquaintances of mine have become Orthodox because of the codes .\n",
      "My acquaintances shied away from becoming Orthodox because of the codes .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 63\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Among runners-up is Boston solo Eleanor Newhoff .\n",
      "Boston solo Eleanor Newhoff won .\n",
      "0\n",
      "Congress ' determination to make agencies accountable for their performance lay at the heart of two landmark reforms of the 1990 the Chief Financial Officers ( CFO ) Act of 1990 and the Government Performance and Results Act of 1993 ( GPRA ) .\n",
      "Congress made several big reforms in the nineties to ensure agencies are held accountable for their actions .\n",
      "1\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 72\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "De Wit worked from likenesses of actual monarchs to produce his portraits .\n",
      "To create his portraits , De Wit used the likenesses of real monarchs .\n",
      "1\n",
      "The rock has a soft texture and can be bought in a variety of shapes .\n",
      "The rock is harder than most types of rock .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre fiction has validation accuracy: 45.527638190954775\n",
      "Genre telephone has validation accuracy: 40.398009950248756\n",
      "Genre slate has validation accuracy: 40.5189620758483\n",
      "Genre government has validation accuracy: 41.43700787401575\n",
      "Genre travel has validation accuracy: 41.64969450101833\n"
     ]
    }
   ],
   "source": [
    "# for each genre, build validation set and evaluate on it. \n",
    "cnn_results = {}\n",
    "model = TwoSentenceModel(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/vrajiv/rnn-cnn-natural-language-inference/best_rnn\"))\n",
    "model.eval()\n",
    "for genre in mnli_val_dict.keys():\n",
    "    sent1_val_indices, sent2_val_indices, val_label = data_pipeline(mnli_val_dict[genre][\"sent1s\"], \n",
    "                                                                    mnli_val_dict[genre][\"sent2s\"], \n",
    "                                                                    mnli_val_dict[genre][\"label\"])\n",
    "    val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             collate_fn=twosentences_collate_func,\n",
    "                                             #shuffle=True\n",
    "                                             )\n",
    "    cnn_results[genre] = test_model(val_loader, model)\n",
    "\n",
    "for genre in mnli_val_dict.keys():\n",
    "    print(\"Genre {} has validation accuracy: {}\".format(genre, cnn_results[genre]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-7e0ce24a816a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindices_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2index_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_token_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'lengths'"
     ]
    }
   ],
   "source": [
    "sample_sent1 = ['How', \"are\", \"you\", \"doing\", \"my\", \"main\", \"man\"]\n",
    "sample_sent2 = ['I', \"am\", \"building\", \"a\", \"bruh\", \"hehe\", \"classifier\"]\n",
    "sample_token_data = [sample_sent1, sample_sent2]\n",
    "\n",
    "indices_data = torch.from_numpy(np.array(token2index_dataset(sample_token_data)))\n",
    "model.forward(indices_data, lengths = torch.from_numpy(np.array([7, 7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent1 = ['How', \"are\", \"you\", \"doing\", \"my\", \"main\", \"man\", \"pad\"]\n",
    "sample_sent2 = ['I', \"am\", \"building\", \"a\", \"bruh\", \"hehe\", \"classifier\"]\n",
    "\n",
    "sample_sent3 = ['How', \"are\", \"you\", \"doing\", \"my\", \"main\", \"man\", \"pad\"]\n",
    "sample_sent4 = ['I', \"am\", \"building\", \"a\", \"bruh\", \"hehe\", \"classifier\"]\n",
    "\n",
    "sample_sent5 = ['How', \"are\", \"you\", \"doing\", \"my\", \"main\", \"man\", \"pad\"]\n",
    "sample_sent6 = ['I', \"am\", \"building\", \"a\", \"bruh\", \"hehe\", \"classifier\"]\n",
    "\n",
    "sample_combined_list = []\n",
    "sample_combined_list.append([sample_sent1, sample_sent2])\n",
    "sample_combined_list.append([sample_sent3, sample_sent4])\n",
    "sample_combined_list.append([sample_sent5, sample_sent6])\n",
    "\n",
    "\n",
    "np.array(sample_combined_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "    print(\"DATA\\n\")\n",
    "    print(data.size())\n",
    "    print (data)\n",
    "    print(sent1_lengths)\n",
    "    print(sent2_lengths)\n",
    "    print (labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Initialize unk vector to * 0.01 and 0.001.  \n",
    "###\n",
    "\n",
    "###\n",
    "### Sort descending before doing pack_padded_sequence\n",
    "###\n",
    "\n",
    "###\n",
    "### Sort all training sentences by descending order. \n",
    "### Feed it into the batch. \n",
    "### Run your model through each of them to get hidden outputs\n",
    "### Match them back again to right order to calculate loss. \n",
    "###\n",
    "\n",
    "###\n",
    "### Freeze the embedding matrix weights so they don't train. \n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
