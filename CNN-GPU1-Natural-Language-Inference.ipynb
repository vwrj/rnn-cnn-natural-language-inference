{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for each step in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    print(n, d)\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "def build_vocab():\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    word_vectors = pkl.load(open(\"fasttext_word_vectors.p\", \"rb\"))\n",
    "    id2token = list(word_vectors.keys())\n",
    "    token2id = dict(zip(word_vectors, range(2,2+len(word_vectors)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return word_vectors, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integers(data_label):\n",
    "    for i in range(len(data_label)):\n",
    "        if data_label[i] == \"contradiction\":\n",
    "            data_label[i] = 0\n",
    "        elif data_label[i] == \"entailment\":\n",
    "            data_label[i] = 1\n",
    "        elif data_label[i] == \"neutral\":\n",
    "            data_label[i] = 2\n",
    "    return data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_order(sent1_data, sent2_data, data_label):\n",
    "    i = random.randint(1, len(sent1_data))\n",
    "    print(sent1_data[i])\n",
    "    print(sent2_data[i])\n",
    "    print(data_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize each entry in a list of sentences\n",
    "def tokenize(sentence_list):\n",
    "    return [word_tokenize(sentence_list[i]) for i in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"one-hot encode\": convert each token to id in vocabulary vector (token2id)\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary & embedding matrix from FastText vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights = np.array(list(word_vectors.values()))\n",
    "pad_vec = np.zeros((1, 300))\n",
    "unk_vec = np.random.randn(1, 300) * 0.01\n",
    "pad_unk_vecs = np.vstack((pad_vec, unk_vec))\n",
    "_WEIGHTS = np.vstack((pad_unk_vecs, _weights))\n",
    "_WEIGHTS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pre-process data for TwoSentenceModel\n",
    "#### Shuffle, word tokenize, one-hot index into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(sent1s, sent2s, labels, verify=True):\n",
    "    labels = convert_labels_to_integers(labels)\n",
    "    seed = random.randint(1, 100)\n",
    "    print(\"Random seed for shuffling: {}\".format(seed))\n",
    "    random.Random(seed).shuffle(sent1s)\n",
    "    random.Random(seed).shuffle(sent2s)\n",
    "    random.Random(seed).shuffle(labels)\n",
    "    \n",
    "    print(\"\\nVerifying that the data and label match after shuffling\")\n",
    "    if verify:\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "          \n",
    "    print(\"\\nTokenizing sentence 1 list...\")    \n",
    "    sent1s_tokenized = tokenize(sent1s)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nTokenizing sentence 2 list... \")  \n",
    "    sent2s_tokenized = tokenize(sent2s)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    print(\"\\nOne-hot encoding words for sentence 1 list...\")  \n",
    "    sent1s_indices = token2index_dataset(sent1s_tokenized)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nOne-hot encoding words for sentence 2 list...\")  \n",
    "    sent2s_indices = token2index_dataset(sent2s_tokenized)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return (sent1s_indices, sent2s_indices, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_data_list, sent2_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param sent1_data_list: list of sentence1's (index matches sentence2's and target_list below)\n",
    "        @param sent2_data_list: list of sentence2's\n",
    "        @param target_list: list of correct labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data_list = sent1_data_list\n",
    "        self.sent2_data_list = sent2_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data_list) == len(self.target_list) and len(self.sent2_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[sentence, 1, tokens], [sentence, 2, tokens]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1_tokens_idx = self.sent1_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_tokens_idx = self.sent2_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        combined_tokens_idx = [sent1_tokens_idx, sent2_tokens_idx]\n",
    "        label = self.target_list[key]\n",
    "        return [combined_tokens_idx, len(sent1_tokens_idx), len(sent2_tokens_idx), label]\n",
    "\n",
    "def twosentences_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_length_list = []\n",
    "    label_list = []\n",
    "    combined_data_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[1])\n",
    "        sent2_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0][0]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[0][1]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        combined_data_list.append([padded_vec_1, padded_vec_2])\n",
    "    return [torch.from_numpy(np.array(combined_data_list)), \n",
    "            torch.LongTensor(sent1_length_list), torch.LongTensor(sent2_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 100000\n"
     ]
    }
   ],
   "source": [
    "snli_train = pd.read_csv('snli_train.tsv', sep='\\t')\n",
    "TRAIN_SIZE = 100000\n",
    "\n",
    "sent1_data = list(snli_train[\"sentence1\"])[:TRAIN_SIZE]\n",
    "sent2_data = list(snli_train[\"sentence2\"])[:TRAIN_SIZE]\n",
    "data_label = list(snli_train[\"label\"])[:TRAIN_SIZE]\n",
    "print(\"Size of training data: {}\".format(len(sent1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 77\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "A few people are packing food into containers and boxes .\n",
      "People are packing food .\n",
      "1\n",
      "A man wearing a Chinese flag sits on a stone monument and plays the banjo .\n",
      "A man plays a guitar while he rollerskates .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Finished creating train_loader.\n"
     ]
    }
   ],
   "source": [
    "sent1_train_indices, sent2_train_indices, train_label = data_pipeline(sent1_data, sent2_data, data_label)\n",
    "train_dataset = TwoSentencesDataset(sent1_train_indices, sent2_train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "print(\"Finished creating train_loader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of val data: 1000\n"
     ]
    }
   ],
   "source": [
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t')\n",
    "sent1_val = list(snli_val[\"sentence1\"])\n",
    "sent2_val = list(snli_val[\"sentence2\"])\n",
    "val_label = list(snli_val[\"label\"])\n",
    "print(\"Size of val data: {}\".format(len(sent1_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 47\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Two young girls looking at Barbie dolls .\n",
      "A girl and a boy are running with toys .\n",
      "0\n",
      "A young boy wearing safety swimming gear and goggles is in a pool .\n",
      "A family of four plays in the surf .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sent1_val_indices, sent2_val_indices, val_label = data_pipeline(sent1_val, sent2_val, val_label)\n",
    "val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in snli_train['sentence1']]).describe()['75%']\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(_WEIGHTS))\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(30)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        seq_len = x.size()[2]\n",
    "        \n",
    "        sent1s = torch.tensor(x[:, 0, :]).cuda()\n",
    "        sent2s = torch.tensor(x[:, 1, :]).cuda()\n",
    "        ordered_sents = torch.cat([sent1s, sent2s], dim=0).cuda()\n",
    "\n",
    "        embed = self.embedding(ordered_sents)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = self.maxpool(hidden.transpose(1, 2)).transpose(1, 2).squeeze(dim=1)\n",
    "        \n",
    "        hidden_sent1s = hidden[0:batch_size, :]\n",
    "        hidden_sent2s = hidden[batch_size:, :]     \n",
    "        \n",
    "        linear1 = self.linear1(torch.cat([hidden_sent1s, hidden_sent2s], dim=1))\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s) + torch.tensor(hidden_sent2s))\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s)*torch.tensor(hidden_sent2s))\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)\n",
    "#         linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8704, -0.7408, -0.3186, -1.2397],\n",
      "        [ 0.9380,  0.4863, -0.6737,  0.1046],\n",
      "        [ 0.4451, -1.6797, -1.1978, -1.1759]])\n",
      "tensor([[ 1.7090,  1.4949, -0.0816,  1.6153],\n",
      "        [-0.0372, -0.5010, -0.4593, -0.2813],\n",
      "        [-0.6716,  0.2631,  1.0884, -0.3128]])\n",
      "tensor([[-1.4875, -1.1074,  0.0260, -2.0025],\n",
      "        [-0.0349, -0.2437,  0.3095, -0.0294],\n",
      "        [-0.2989, -0.4419, -1.3037,  0.3678]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, sent1_lengths, sent2_lengths, labels) in loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.cuda(), sent1_lengths.cuda(), sent2_lengths.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels.cuda()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 7, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    max_val_acc = 0\n",
    "    losses = []\n",
    "    xs = 0\n",
    "    val_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.cuda(), sent1_lengths.cuda(), sent2_lengths.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, sent1_length_batch, sent2_length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                val_accs.append(val_acc)\n",
    "                xs += 100\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "                \n",
    "    print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc, losses, xs, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training (loss) and validation (accuracy) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_validation_curves(losses, xs, val_accs, title):\n",
    "    fig = plt.figure(figsize=(4, 3))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    loss_avg_vals = []\n",
    "    for i in range(0, len(losses)-100, 100):\n",
    "        s = 0\n",
    "        avg = 0\n",
    "        for j in range(i, i+100):\n",
    "            s += losses[j]\n",
    "        avg = s/100.0\n",
    "        loss_avg_vals.append(avg)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(len(loss_avg_vals)), loss_avg_vals)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Average Train Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(0, xs, 100), val_accs)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Val Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/7], Step: [101/3125], Validation Acc: 35.8\n",
      "Epoch: [1/7], Step: [101/3125], Training Loss: 1.101027011871338\n",
      "Epoch: [1/7], Step: [201/3125], Validation Acc: 36.3\n",
      "Epoch: [1/7], Step: [201/3125], Training Loss: 1.0916448831558228\n",
      "Epoch: [1/7], Step: [301/3125], Validation Acc: 37.7\n",
      "Epoch: [1/7], Step: [301/3125], Training Loss: 1.1033012866973877\n",
      "Epoch: [1/7], Step: [401/3125], Validation Acc: 38.6\n",
      "Epoch: [1/7], Step: [401/3125], Training Loss: 1.0696437358856201\n",
      "Epoch: [1/7], Step: [501/3125], Validation Acc: 41.5\n",
      "Epoch: [1/7], Step: [501/3125], Training Loss: 1.0386451482772827\n",
      "Epoch: [1/7], Step: [601/3125], Validation Acc: 41.8\n",
      "Epoch: [1/7], Step: [601/3125], Training Loss: 1.074448585510254\n",
      "Epoch: [1/7], Step: [701/3125], Validation Acc: 43.4\n",
      "Epoch: [1/7], Step: [701/3125], Training Loss: 1.0154290199279785\n",
      "Epoch: [1/7], Step: [801/3125], Validation Acc: 45.2\n",
      "Epoch: [1/7], Step: [801/3125], Training Loss: 1.0488476753234863\n",
      "Epoch: [1/7], Step: [901/3125], Validation Acc: 45.2\n",
      "Epoch: [1/7], Step: [901/3125], Training Loss: 0.9734014868736267\n",
      "Epoch: [1/7], Step: [1001/3125], Validation Acc: 44.8\n",
      "Epoch: [1/7], Step: [1001/3125], Training Loss: 1.0399885177612305\n",
      "Epoch: [1/7], Step: [1101/3125], Validation Acc: 47.3\n",
      "Epoch: [1/7], Step: [1101/3125], Training Loss: 1.0026776790618896\n",
      "Epoch: [1/7], Step: [1201/3125], Validation Acc: 48.0\n",
      "Epoch: [1/7], Step: [1201/3125], Training Loss: 0.9839237332344055\n",
      "Epoch: [1/7], Step: [1301/3125], Validation Acc: 47.9\n",
      "Epoch: [1/7], Step: [1301/3125], Training Loss: 0.9890618324279785\n",
      "Epoch: [1/7], Step: [1401/3125], Validation Acc: 48.6\n",
      "Epoch: [1/7], Step: [1401/3125], Training Loss: 1.0170855522155762\n",
      "Epoch: [1/7], Step: [1501/3125], Validation Acc: 47.6\n",
      "Epoch: [1/7], Step: [1501/3125], Training Loss: 0.9194089770317078\n",
      "Epoch: [1/7], Step: [1601/3125], Validation Acc: 51.3\n",
      "Epoch: [1/7], Step: [1601/3125], Training Loss: 0.9253873229026794\n",
      "Epoch: [1/7], Step: [1701/3125], Validation Acc: 50.0\n",
      "Epoch: [1/7], Step: [1701/3125], Training Loss: 0.9909142255783081\n",
      "Epoch: [1/7], Step: [1801/3125], Validation Acc: 51.9\n",
      "Epoch: [1/7], Step: [1801/3125], Training Loss: 0.9881094098091125\n",
      "Epoch: [1/7], Step: [1901/3125], Validation Acc: 51.6\n",
      "Epoch: [1/7], Step: [1901/3125], Training Loss: 0.7908936738967896\n",
      "Epoch: [1/7], Step: [2001/3125], Validation Acc: 51.3\n",
      "Epoch: [1/7], Step: [2001/3125], Training Loss: 1.005164623260498\n",
      "Epoch: [1/7], Step: [2101/3125], Validation Acc: 54.1\n",
      "Epoch: [1/7], Step: [2101/3125], Training Loss: 0.8411334156990051\n",
      "Epoch: [1/7], Step: [2201/3125], Validation Acc: 51.4\n",
      "Epoch: [1/7], Step: [2201/3125], Training Loss: 0.8719980120658875\n",
      "Epoch: [1/7], Step: [2301/3125], Validation Acc: 53.4\n",
      "Epoch: [1/7], Step: [2301/3125], Training Loss: 1.0785737037658691\n",
      "Epoch: [1/7], Step: [2401/3125], Validation Acc: 51.6\n",
      "Epoch: [1/7], Step: [2401/3125], Training Loss: 0.9303179383277893\n",
      "Epoch: [1/7], Step: [2501/3125], Validation Acc: 52.7\n",
      "Epoch: [1/7], Step: [2501/3125], Training Loss: 0.8251590132713318\n",
      "Epoch: [1/7], Step: [2601/3125], Validation Acc: 54.0\n",
      "Epoch: [1/7], Step: [2601/3125], Training Loss: 0.8224403858184814\n",
      "Epoch: [1/7], Step: [2701/3125], Validation Acc: 55.2\n",
      "Epoch: [1/7], Step: [2701/3125], Training Loss: 1.0092138051986694\n",
      "Epoch: [1/7], Step: [2801/3125], Validation Acc: 55.8\n",
      "Epoch: [1/7], Step: [2801/3125], Training Loss: 0.8446494340896606\n",
      "Epoch: [1/7], Step: [2901/3125], Validation Acc: 54.2\n",
      "Epoch: [1/7], Step: [2901/3125], Training Loss: 0.9965748190879822\n",
      "Epoch: [1/7], Step: [3001/3125], Validation Acc: 55.4\n",
      "Epoch: [1/7], Step: [3001/3125], Training Loss: 0.957108199596405\n",
      "Epoch: [1/7], Step: [3101/3125], Validation Acc: 55.1\n",
      "Epoch: [1/7], Step: [3101/3125], Training Loss: 0.9204617738723755\n",
      "Epoch: [2/7], Step: [101/3125], Validation Acc: 55.2\n",
      "Epoch: [2/7], Step: [101/3125], Training Loss: 0.8339826464653015\n",
      "Epoch: [2/7], Step: [201/3125], Validation Acc: 56.1\n",
      "Epoch: [2/7], Step: [201/3125], Training Loss: 0.9147244095802307\n",
      "Epoch: [2/7], Step: [301/3125], Validation Acc: 54.4\n",
      "Epoch: [2/7], Step: [301/3125], Training Loss: 0.921146810054779\n",
      "Epoch: [2/7], Step: [401/3125], Validation Acc: 54.9\n",
      "Epoch: [2/7], Step: [401/3125], Training Loss: 0.9763403534889221\n",
      "Epoch: [2/7], Step: [501/3125], Validation Acc: 56.2\n",
      "Epoch: [2/7], Step: [501/3125], Training Loss: 0.8752696514129639\n",
      "Epoch: [2/7], Step: [601/3125], Validation Acc: 56.8\n",
      "Epoch: [2/7], Step: [601/3125], Training Loss: 1.0078884363174438\n",
      "Epoch: [2/7], Step: [701/3125], Validation Acc: 58.3\n",
      "Epoch: [2/7], Step: [701/3125], Training Loss: 0.8899440169334412\n",
      "Epoch: [2/7], Step: [801/3125], Validation Acc: 58.3\n",
      "Epoch: [2/7], Step: [801/3125], Training Loss: 0.6775332689285278\n",
      "Epoch: [2/7], Step: [901/3125], Validation Acc: 59.0\n",
      "Epoch: [2/7], Step: [901/3125], Training Loss: 0.8894573450088501\n",
      "Epoch: [2/7], Step: [1001/3125], Validation Acc: 58.5\n",
      "Epoch: [2/7], Step: [1001/3125], Training Loss: 0.9518184661865234\n",
      "Epoch: [2/7], Step: [1101/3125], Validation Acc: 60.3\n",
      "Epoch: [2/7], Step: [1101/3125], Training Loss: 0.8281195759773254\n",
      "Epoch: [2/7], Step: [1201/3125], Validation Acc: 57.4\n",
      "Epoch: [2/7], Step: [1201/3125], Training Loss: 0.9094740748405457\n",
      "Epoch: [2/7], Step: [1301/3125], Validation Acc: 58.7\n",
      "Epoch: [2/7], Step: [1301/3125], Training Loss: 0.8987275958061218\n",
      "Epoch: [2/7], Step: [1401/3125], Validation Acc: 58.6\n",
      "Epoch: [2/7], Step: [1401/3125], Training Loss: 0.7752294540405273\n",
      "Epoch: [2/7], Step: [1501/3125], Validation Acc: 59.8\n",
      "Epoch: [2/7], Step: [1501/3125], Training Loss: 0.7842450141906738\n",
      "Epoch: [2/7], Step: [1601/3125], Validation Acc: 60.4\n",
      "Epoch: [2/7], Step: [1601/3125], Training Loss: 0.8422006368637085\n",
      "Epoch: [2/7], Step: [1701/3125], Validation Acc: 58.0\n",
      "Epoch: [2/7], Step: [1701/3125], Training Loss: 0.8927692174911499\n",
      "Epoch: [2/7], Step: [1801/3125], Validation Acc: 57.7\n",
      "Epoch: [2/7], Step: [1801/3125], Training Loss: 0.897503137588501\n",
      "Epoch: [2/7], Step: [1901/3125], Validation Acc: 58.6\n",
      "Epoch: [2/7], Step: [1901/3125], Training Loss: 0.7047228813171387\n",
      "Epoch: [2/7], Step: [2001/3125], Validation Acc: 58.5\n",
      "Epoch: [2/7], Step: [2001/3125], Training Loss: 0.8738075494766235\n",
      "Epoch: [2/7], Step: [2101/3125], Validation Acc: 58.6\n",
      "Epoch: [2/7], Step: [2101/3125], Training Loss: 0.8287938833236694\n",
      "Epoch: [2/7], Step: [2201/3125], Validation Acc: 59.4\n",
      "Epoch: [2/7], Step: [2201/3125], Training Loss: 0.794831395149231\n",
      "Epoch: [2/7], Step: [2301/3125], Validation Acc: 60.6\n",
      "Epoch: [2/7], Step: [2301/3125], Training Loss: 1.105830430984497\n",
      "Epoch: [2/7], Step: [2401/3125], Validation Acc: 58.3\n",
      "Epoch: [2/7], Step: [2401/3125], Training Loss: 0.8078869581222534\n",
      "Epoch: [2/7], Step: [2501/3125], Validation Acc: 60.1\n",
      "Epoch: [2/7], Step: [2501/3125], Training Loss: 0.7476287484169006\n",
      "Epoch: [2/7], Step: [2601/3125], Validation Acc: 57.5\n",
      "Epoch: [2/7], Step: [2601/3125], Training Loss: 0.7598414421081543\n",
      "Epoch: [2/7], Step: [2701/3125], Validation Acc: 59.0\n",
      "Epoch: [2/7], Step: [2701/3125], Training Loss: 0.9054904580116272\n",
      "Epoch: [2/7], Step: [2801/3125], Validation Acc: 61.8\n",
      "Epoch: [2/7], Step: [2801/3125], Training Loss: 0.9064485430717468\n",
      "Epoch: [2/7], Step: [2901/3125], Validation Acc: 59.9\n",
      "Epoch: [2/7], Step: [2901/3125], Training Loss: 0.8700875043869019\n",
      "Epoch: [2/7], Step: [3001/3125], Validation Acc: 61.7\n",
      "Epoch: [2/7], Step: [3001/3125], Training Loss: 0.8681102991104126\n",
      "Epoch: [2/7], Step: [3101/3125], Validation Acc: 60.8\n",
      "Epoch: [2/7], Step: [3101/3125], Training Loss: 0.7846904993057251\n",
      "Epoch: [3/7], Step: [101/3125], Validation Acc: 59.3\n",
      "Epoch: [3/7], Step: [101/3125], Training Loss: 0.7921625375747681\n",
      "Epoch: [3/7], Step: [201/3125], Validation Acc: 59.6\n",
      "Epoch: [3/7], Step: [201/3125], Training Loss: 0.8014023303985596\n",
      "Epoch: [3/7], Step: [301/3125], Validation Acc: 58.5\n",
      "Epoch: [3/7], Step: [301/3125], Training Loss: 0.7875455617904663\n",
      "Epoch: [3/7], Step: [401/3125], Validation Acc: 59.3\n",
      "Epoch: [3/7], Step: [401/3125], Training Loss: 0.8849825263023376\n",
      "Epoch: [3/7], Step: [501/3125], Validation Acc: 60.3\n",
      "Epoch: [3/7], Step: [501/3125], Training Loss: 0.7634339928627014\n",
      "Epoch: [3/7], Step: [601/3125], Validation Acc: 61.0\n",
      "Epoch: [3/7], Step: [601/3125], Training Loss: 0.936095654964447\n",
      "Epoch: [3/7], Step: [701/3125], Validation Acc: 62.8\n",
      "Epoch: [3/7], Step: [701/3125], Training Loss: 0.8393616676330566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/7], Step: [801/3125], Validation Acc: 62.0\n",
      "Epoch: [3/7], Step: [801/3125], Training Loss: 0.621756374835968\n",
      "Epoch: [3/7], Step: [901/3125], Validation Acc: 61.8\n",
      "Epoch: [3/7], Step: [901/3125], Training Loss: 0.8702166080474854\n",
      "Epoch: [3/7], Step: [1001/3125], Validation Acc: 61.3\n",
      "Epoch: [3/7], Step: [1001/3125], Training Loss: 0.8829164505004883\n",
      "Epoch: [3/7], Step: [1101/3125], Validation Acc: 62.0\n",
      "Epoch: [3/7], Step: [1101/3125], Training Loss: 0.7650295495986938\n",
      "Epoch: [3/7], Step: [1201/3125], Validation Acc: 60.8\n",
      "Epoch: [3/7], Step: [1201/3125], Training Loss: 0.8357694745063782\n",
      "Epoch: [3/7], Step: [1301/3125], Validation Acc: 60.6\n",
      "Epoch: [3/7], Step: [1301/3125], Training Loss: 0.7405416369438171\n",
      "Epoch: [3/7], Step: [1401/3125], Validation Acc: 61.4\n",
      "Epoch: [3/7], Step: [1401/3125], Training Loss: 0.7267556190490723\n",
      "Epoch: [3/7], Step: [1501/3125], Validation Acc: 60.2\n",
      "Epoch: [3/7], Step: [1501/3125], Training Loss: 0.7373424172401428\n",
      "Epoch: [3/7], Step: [1601/3125], Validation Acc: 60.5\n",
      "Epoch: [3/7], Step: [1601/3125], Training Loss: 0.7787948846817017\n",
      "Epoch: [3/7], Step: [1701/3125], Validation Acc: 60.5\n",
      "Epoch: [3/7], Step: [1701/3125], Training Loss: 0.7760307788848877\n",
      "Epoch: [3/7], Step: [1801/3125], Validation Acc: 59.3\n",
      "Epoch: [3/7], Step: [1801/3125], Training Loss: 0.8860973715782166\n",
      "Epoch: [3/7], Step: [1901/3125], Validation Acc: 60.0\n",
      "Epoch: [3/7], Step: [1901/3125], Training Loss: 0.6168725490570068\n",
      "Epoch: [3/7], Step: [2001/3125], Validation Acc: 59.7\n",
      "Epoch: [3/7], Step: [2001/3125], Training Loss: 0.855000913143158\n",
      "Epoch: [3/7], Step: [2101/3125], Validation Acc: 61.2\n",
      "Epoch: [3/7], Step: [2101/3125], Training Loss: 0.7855597734451294\n",
      "Epoch: [3/7], Step: [2201/3125], Validation Acc: 61.4\n",
      "Epoch: [3/7], Step: [2201/3125], Training Loss: 0.8385024070739746\n",
      "Epoch: [3/7], Step: [2301/3125], Validation Acc: 61.0\n",
      "Epoch: [3/7], Step: [2301/3125], Training Loss: 1.0646063089370728\n",
      "Epoch: [3/7], Step: [2401/3125], Validation Acc: 60.5\n",
      "Epoch: [3/7], Step: [2401/3125], Training Loss: 0.7606577277183533\n",
      "Epoch: [3/7], Step: [2501/3125], Validation Acc: 60.7\n",
      "Epoch: [3/7], Step: [2501/3125], Training Loss: 0.714949369430542\n",
      "Epoch: [3/7], Step: [2601/3125], Validation Acc: 60.3\n",
      "Epoch: [3/7], Step: [2601/3125], Training Loss: 0.6785979270935059\n",
      "Epoch: [3/7], Step: [2701/3125], Validation Acc: 60.9\n",
      "Epoch: [3/7], Step: [2701/3125], Training Loss: 0.8898694515228271\n",
      "Epoch: [3/7], Step: [2801/3125], Validation Acc: 61.2\n",
      "Epoch: [3/7], Step: [2801/3125], Training Loss: 0.8844815492630005\n",
      "Epoch: [3/7], Step: [2901/3125], Validation Acc: 61.1\n",
      "Epoch: [3/7], Step: [2901/3125], Training Loss: 0.7984758019447327\n",
      "Epoch: [3/7], Step: [3001/3125], Validation Acc: 61.8\n",
      "Epoch: [3/7], Step: [3001/3125], Training Loss: 0.8536452054977417\n",
      "Epoch: [3/7], Step: [3101/3125], Validation Acc: 62.1\n",
      "Epoch: [3/7], Step: [3101/3125], Training Loss: 0.7460243105888367\n",
      "Epoch: [4/7], Step: [101/3125], Validation Acc: 61.4\n",
      "Epoch: [4/7], Step: [101/3125], Training Loss: 0.76646488904953\n",
      "Epoch: [4/7], Step: [201/3125], Validation Acc: 61.2\n",
      "Epoch: [4/7], Step: [201/3125], Training Loss: 0.7164719104766846\n",
      "Epoch: [4/7], Step: [301/3125], Validation Acc: 61.1\n",
      "Epoch: [4/7], Step: [301/3125], Training Loss: 0.7386553287506104\n",
      "Epoch: [4/7], Step: [401/3125], Validation Acc: 60.6\n",
      "Epoch: [4/7], Step: [401/3125], Training Loss: 0.7889318466186523\n",
      "Epoch: [4/7], Step: [501/3125], Validation Acc: 62.2\n",
      "Epoch: [4/7], Step: [501/3125], Training Loss: 0.7150264382362366\n",
      "Epoch: [4/7], Step: [601/3125], Validation Acc: 61.1\n",
      "Epoch: [4/7], Step: [601/3125], Training Loss: 0.8759336471557617\n",
      "Epoch: [4/7], Step: [701/3125], Validation Acc: 63.2\n",
      "Epoch: [4/7], Step: [701/3125], Training Loss: 0.8067615032196045\n",
      "Epoch: [4/7], Step: [801/3125], Validation Acc: 61.5\n",
      "Epoch: [4/7], Step: [801/3125], Training Loss: 0.5958108901977539\n",
      "Epoch: [4/7], Step: [901/3125], Validation Acc: 62.0\n",
      "Epoch: [4/7], Step: [901/3125], Training Loss: 0.8361579775810242\n",
      "Epoch: [4/7], Step: [1001/3125], Validation Acc: 62.9\n",
      "Epoch: [4/7], Step: [1001/3125], Training Loss: 0.8743258714675903\n",
      "Epoch: [4/7], Step: [1101/3125], Validation Acc: 62.7\n",
      "Epoch: [4/7], Step: [1101/3125], Training Loss: 0.7330713868141174\n",
      "Epoch: [4/7], Step: [1201/3125], Validation Acc: 61.5\n",
      "Epoch: [4/7], Step: [1201/3125], Training Loss: 0.7865010499954224\n",
      "Epoch: [4/7], Step: [1301/3125], Validation Acc: 63.0\n",
      "Epoch: [4/7], Step: [1301/3125], Training Loss: 0.7024917006492615\n",
      "Epoch: [4/7], Step: [1401/3125], Validation Acc: 61.3\n",
      "Epoch: [4/7], Step: [1401/3125], Training Loss: 0.7192803621292114\n",
      "Epoch: [4/7], Step: [1501/3125], Validation Acc: 62.4\n",
      "Epoch: [4/7], Step: [1501/3125], Training Loss: 0.7038572430610657\n",
      "Epoch: [4/7], Step: [1601/3125], Validation Acc: 61.4\n",
      "Epoch: [4/7], Step: [1601/3125], Training Loss: 0.7201718688011169\n",
      "Epoch: [4/7], Step: [1701/3125], Validation Acc: 61.7\n",
      "Epoch: [4/7], Step: [1701/3125], Training Loss: 0.7436650991439819\n",
      "Epoch: [4/7], Step: [1801/3125], Validation Acc: 61.3\n",
      "Epoch: [4/7], Step: [1801/3125], Training Loss: 0.7721407413482666\n",
      "Epoch: [4/7], Step: [1901/3125], Validation Acc: 61.1\n",
      "Epoch: [4/7], Step: [1901/3125], Training Loss: 0.5883822441101074\n",
      "Epoch: [4/7], Step: [2001/3125], Validation Acc: 61.7\n",
      "Epoch: [4/7], Step: [2001/3125], Training Loss: 0.7786887288093567\n",
      "Epoch: [4/7], Step: [2101/3125], Validation Acc: 62.3\n",
      "Epoch: [4/7], Step: [2101/3125], Training Loss: 0.7313192486763\n",
      "Epoch: [4/7], Step: [2201/3125], Validation Acc: 62.1\n",
      "Epoch: [4/7], Step: [2201/3125], Training Loss: 0.7251425385475159\n",
      "Epoch: [4/7], Step: [2301/3125], Validation Acc: 63.2\n",
      "Epoch: [4/7], Step: [2301/3125], Training Loss: 1.0233497619628906\n",
      "Epoch: [4/7], Step: [2401/3125], Validation Acc: 61.9\n",
      "Epoch: [4/7], Step: [2401/3125], Training Loss: 0.722163200378418\n",
      "Epoch: [4/7], Step: [2501/3125], Validation Acc: 60.9\n",
      "Epoch: [4/7], Step: [2501/3125], Training Loss: 0.6578465700149536\n",
      "Epoch: [4/7], Step: [2601/3125], Validation Acc: 60.8\n",
      "Epoch: [4/7], Step: [2601/3125], Training Loss: 0.6445702910423279\n",
      "Epoch: [4/7], Step: [2701/3125], Validation Acc: 61.0\n",
      "Epoch: [4/7], Step: [2701/3125], Training Loss: 0.8243350386619568\n",
      "Epoch: [4/7], Step: [2801/3125], Validation Acc: 62.7\n",
      "Epoch: [4/7], Step: [2801/3125], Training Loss: 0.8409488201141357\n",
      "Epoch: [4/7], Step: [2901/3125], Validation Acc: 61.5\n",
      "Epoch: [4/7], Step: [2901/3125], Training Loss: 0.7823771834373474\n",
      "Epoch: [4/7], Step: [3001/3125], Validation Acc: 63.2\n",
      "Epoch: [4/7], Step: [3001/3125], Training Loss: 0.7734310030937195\n",
      "Epoch: [4/7], Step: [3101/3125], Validation Acc: 64.1\n",
      "Epoch: [4/7], Step: [3101/3125], Training Loss: 0.7183501720428467\n",
      "Epoch: [5/7], Step: [101/3125], Validation Acc: 62.3\n",
      "Epoch: [5/7], Step: [101/3125], Training Loss: 0.6825486421585083\n",
      "Epoch: [5/7], Step: [201/3125], Validation Acc: 61.7\n",
      "Epoch: [5/7], Step: [201/3125], Training Loss: 0.6791492104530334\n",
      "Epoch: [5/7], Step: [301/3125], Validation Acc: 62.3\n",
      "Epoch: [5/7], Step: [301/3125], Training Loss: 0.6905837059020996\n",
      "Epoch: [5/7], Step: [401/3125], Validation Acc: 61.3\n",
      "Epoch: [5/7], Step: [401/3125], Training Loss: 0.7583428621292114\n",
      "Epoch: [5/7], Step: [501/3125], Validation Acc: 61.9\n",
      "Epoch: [5/7], Step: [501/3125], Training Loss: 0.720137894153595\n",
      "Epoch: [5/7], Step: [601/3125], Validation Acc: 63.2\n",
      "Epoch: [5/7], Step: [601/3125], Training Loss: 0.7835221886634827\n",
      "Epoch: [5/7], Step: [701/3125], Validation Acc: 61.7\n",
      "Epoch: [5/7], Step: [701/3125], Training Loss: 0.8562607765197754\n",
      "Epoch: [5/7], Step: [801/3125], Validation Acc: 61.5\n",
      "Epoch: [5/7], Step: [801/3125], Training Loss: 0.5404738783836365\n",
      "Epoch: [5/7], Step: [901/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [901/3125], Training Loss: 0.8405516743659973\n",
      "Epoch: [5/7], Step: [1001/3125], Validation Acc: 61.9\n",
      "Epoch: [5/7], Step: [1001/3125], Training Loss: 0.8534097671508789\n",
      "Epoch: [5/7], Step: [1101/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [1101/3125], Training Loss: 0.6365213990211487\n",
      "Epoch: [5/7], Step: [1201/3125], Validation Acc: 61.4\n",
      "Epoch: [5/7], Step: [1201/3125], Training Loss: 0.7296717762947083\n",
      "Epoch: [5/7], Step: [1301/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [1301/3125], Training Loss: 0.7950129508972168\n",
      "Epoch: [5/7], Step: [1401/3125], Validation Acc: 62.0\n",
      "Epoch: [5/7], Step: [1401/3125], Training Loss: 0.7327640056610107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/7], Step: [1501/3125], Validation Acc: 61.1\n",
      "Epoch: [5/7], Step: [1501/3125], Training Loss: 0.5832830667495728\n",
      "Epoch: [5/7], Step: [1601/3125], Validation Acc: 61.4\n",
      "Epoch: [5/7], Step: [1601/3125], Training Loss: 0.6622936129570007\n",
      "Epoch: [5/7], Step: [1701/3125], Validation Acc: 60.4\n",
      "Epoch: [5/7], Step: [1701/3125], Training Loss: 0.7139739990234375\n",
      "Epoch: [5/7], Step: [1801/3125], Validation Acc: 61.4\n",
      "Epoch: [5/7], Step: [1801/3125], Training Loss: 0.7312016487121582\n",
      "Epoch: [5/7], Step: [1901/3125], Validation Acc: 62.2\n",
      "Epoch: [5/7], Step: [1901/3125], Training Loss: 0.5293908715248108\n",
      "Epoch: [5/7], Step: [2001/3125], Validation Acc: 61.1\n",
      "Epoch: [5/7], Step: [2001/3125], Training Loss: 0.7708113789558411\n",
      "Epoch: [5/7], Step: [2101/3125], Validation Acc: 62.2\n",
      "Epoch: [5/7], Step: [2101/3125], Training Loss: 0.691259503364563\n",
      "Epoch: [5/7], Step: [2201/3125], Validation Acc: 62.3\n",
      "Epoch: [5/7], Step: [2201/3125], Training Loss: 0.6741315722465515\n",
      "Epoch: [5/7], Step: [2301/3125], Validation Acc: 62.3\n",
      "Epoch: [5/7], Step: [2301/3125], Training Loss: 0.942440927028656\n",
      "Epoch: [5/7], Step: [2401/3125], Validation Acc: 62.2\n",
      "Epoch: [5/7], Step: [2401/3125], Training Loss: 0.7193584442138672\n",
      "Epoch: [5/7], Step: [2501/3125], Validation Acc: 61.7\n",
      "Epoch: [5/7], Step: [2501/3125], Training Loss: 0.6189267635345459\n",
      "Epoch: [5/7], Step: [2601/3125], Validation Acc: 60.2\n",
      "Epoch: [5/7], Step: [2601/3125], Training Loss: 0.6519050002098083\n",
      "Epoch: [5/7], Step: [2701/3125], Validation Acc: 62.2\n",
      "Epoch: [5/7], Step: [2701/3125], Training Loss: 0.7853168845176697\n",
      "Epoch: [5/7], Step: [2801/3125], Validation Acc: 62.4\n",
      "Epoch: [5/7], Step: [2801/3125], Training Loss: 0.8168966770172119\n",
      "Epoch: [5/7], Step: [2901/3125], Validation Acc: 60.5\n",
      "Epoch: [5/7], Step: [2901/3125], Training Loss: 0.7419335246086121\n",
      "Epoch: [5/7], Step: [3001/3125], Validation Acc: 64.1\n",
      "Epoch: [5/7], Step: [3001/3125], Training Loss: 0.7752574682235718\n",
      "Epoch: [5/7], Step: [3101/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [3101/3125], Training Loss: 0.697730302810669\n",
      "Epoch: [6/7], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [6/7], Step: [101/3125], Training Loss: 0.6060791015625\n",
      "Epoch: [6/7], Step: [201/3125], Validation Acc: 62.4\n",
      "Epoch: [6/7], Step: [201/3125], Training Loss: 0.632866621017456\n",
      "Epoch: [6/7], Step: [301/3125], Validation Acc: 61.4\n",
      "Epoch: [6/7], Step: [301/3125], Training Loss: 0.6349664330482483\n",
      "Epoch: [6/7], Step: [401/3125], Validation Acc: 62.5\n",
      "Epoch: [6/7], Step: [401/3125], Training Loss: 0.827633261680603\n",
      "Epoch: [6/7], Step: [501/3125], Validation Acc: 63.6\n",
      "Epoch: [6/7], Step: [501/3125], Training Loss: 0.6629877090454102\n",
      "Epoch: [6/7], Step: [601/3125], Validation Acc: 61.2\n",
      "Epoch: [6/7], Step: [601/3125], Training Loss: 0.7454774379730225\n",
      "Epoch: [6/7], Step: [701/3125], Validation Acc: 63.0\n",
      "Epoch: [6/7], Step: [701/3125], Training Loss: 0.8591675162315369\n",
      "Epoch: [6/7], Step: [801/3125], Validation Acc: 61.8\n",
      "Epoch: [6/7], Step: [801/3125], Training Loss: 0.52008056640625\n",
      "Epoch: [6/7], Step: [901/3125], Validation Acc: 61.8\n",
      "Epoch: [6/7], Step: [901/3125], Training Loss: 0.751274585723877\n",
      "Epoch: [6/7], Step: [1001/3125], Validation Acc: 61.6\n",
      "Epoch: [6/7], Step: [1001/3125], Training Loss: 0.8614146113395691\n",
      "Epoch: [6/7], Step: [1101/3125], Validation Acc: 63.1\n",
      "Epoch: [6/7], Step: [1101/3125], Training Loss: 0.572192907333374\n",
      "Epoch: [6/7], Step: [1201/3125], Validation Acc: 61.9\n",
      "Epoch: [6/7], Step: [1201/3125], Training Loss: 0.6423856616020203\n",
      "Epoch: [6/7], Step: [1301/3125], Validation Acc: 61.4\n",
      "Epoch: [6/7], Step: [1301/3125], Training Loss: 0.6861497163772583\n",
      "Epoch: [6/7], Step: [1401/3125], Validation Acc: 61.8\n",
      "Epoch: [6/7], Step: [1401/3125], Training Loss: 0.6680733561515808\n",
      "Epoch: [6/7], Step: [1501/3125], Validation Acc: 62.1\n",
      "Epoch: [6/7], Step: [1501/3125], Training Loss: 0.5425317287445068\n",
      "Epoch: [6/7], Step: [1601/3125], Validation Acc: 61.1\n",
      "Epoch: [6/7], Step: [1601/3125], Training Loss: 0.6221718192100525\n",
      "Epoch: [6/7], Step: [1701/3125], Validation Acc: 60.6\n",
      "Epoch: [6/7], Step: [1701/3125], Training Loss: 0.6860573291778564\n",
      "Epoch: [6/7], Step: [1801/3125], Validation Acc: 60.7\n",
      "Epoch: [6/7], Step: [1801/3125], Training Loss: 0.7432286739349365\n",
      "Epoch: [6/7], Step: [1901/3125], Validation Acc: 61.3\n",
      "Epoch: [6/7], Step: [1901/3125], Training Loss: 0.4601258635520935\n",
      "Epoch: [6/7], Step: [2001/3125], Validation Acc: 61.2\n",
      "Epoch: [6/7], Step: [2001/3125], Training Loss: 0.7047117948532104\n",
      "Epoch: [6/7], Step: [2101/3125], Validation Acc: 62.8\n",
      "Epoch: [6/7], Step: [2101/3125], Training Loss: 0.6263083219528198\n",
      "Epoch: [6/7], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [6/7], Step: [2201/3125], Training Loss: 0.6726021766662598\n",
      "Epoch: [6/7], Step: [2301/3125], Validation Acc: 62.8\n",
      "Epoch: [6/7], Step: [2301/3125], Training Loss: 0.8658004403114319\n",
      "Epoch: [6/7], Step: [2401/3125], Validation Acc: 60.7\n",
      "Epoch: [6/7], Step: [2401/3125], Training Loss: 0.6963496208190918\n",
      "Epoch: [6/7], Step: [2501/3125], Validation Acc: 62.6\n",
      "Epoch: [6/7], Step: [2501/3125], Training Loss: 0.545699417591095\n",
      "Epoch: [6/7], Step: [2601/3125], Validation Acc: 62.1\n",
      "Epoch: [6/7], Step: [2601/3125], Training Loss: 0.6048126220703125\n",
      "Epoch: [6/7], Step: [2701/3125], Validation Acc: 61.5\n",
      "Epoch: [6/7], Step: [2701/3125], Training Loss: 0.7425341606140137\n",
      "Epoch: [6/7], Step: [2801/3125], Validation Acc: 62.6\n",
      "Epoch: [6/7], Step: [2801/3125], Training Loss: 0.7611191272735596\n",
      "Epoch: [6/7], Step: [2901/3125], Validation Acc: 60.6\n",
      "Epoch: [6/7], Step: [2901/3125], Training Loss: 0.7087089419364929\n",
      "Epoch: [6/7], Step: [3001/3125], Validation Acc: 64.4\n",
      "Epoch: [6/7], Step: [3001/3125], Training Loss: 0.7527657747268677\n",
      "Epoch: [6/7], Step: [3101/3125], Validation Acc: 62.9\n",
      "Epoch: [6/7], Step: [3101/3125], Training Loss: 0.6876818537712097\n",
      "Epoch: [7/7], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [7/7], Step: [101/3125], Training Loss: 0.6060709357261658\n",
      "Epoch: [7/7], Step: [201/3125], Validation Acc: 63.1\n",
      "Epoch: [7/7], Step: [201/3125], Training Loss: 0.5697755813598633\n",
      "Epoch: [7/7], Step: [301/3125], Validation Acc: 62.6\n",
      "Epoch: [7/7], Step: [301/3125], Training Loss: 0.6558297872543335\n",
      "Epoch: [7/7], Step: [401/3125], Validation Acc: 62.6\n",
      "Epoch: [7/7], Step: [401/3125], Training Loss: 0.7689616680145264\n",
      "Epoch: [7/7], Step: [501/3125], Validation Acc: 63.3\n",
      "Epoch: [7/7], Step: [501/3125], Training Loss: 0.6305052042007446\n",
      "Epoch: [7/7], Step: [601/3125], Validation Acc: 62.1\n",
      "Epoch: [7/7], Step: [601/3125], Training Loss: 0.7447726130485535\n",
      "Epoch: [7/7], Step: [701/3125], Validation Acc: 63.2\n",
      "Epoch: [7/7], Step: [701/3125], Training Loss: 0.7898695468902588\n",
      "Epoch: [7/7], Step: [801/3125], Validation Acc: 62.5\n",
      "Epoch: [7/7], Step: [801/3125], Training Loss: 0.5166373252868652\n",
      "Epoch: [7/7], Step: [901/3125], Validation Acc: 61.8\n",
      "Epoch: [7/7], Step: [901/3125], Training Loss: 0.7600900530815125\n",
      "Epoch: [7/7], Step: [1001/3125], Validation Acc: 62.5\n",
      "Epoch: [7/7], Step: [1001/3125], Training Loss: 0.8046379089355469\n",
      "Epoch: [7/7], Step: [1101/3125], Validation Acc: 63.3\n",
      "Epoch: [7/7], Step: [1101/3125], Training Loss: 0.4323515295982361\n",
      "Epoch: [7/7], Step: [1201/3125], Validation Acc: 63.1\n",
      "Epoch: [7/7], Step: [1201/3125], Training Loss: 0.6148824095726013\n",
      "Epoch: [7/7], Step: [1301/3125], Validation Acc: 61.8\n",
      "Epoch: [7/7], Step: [1301/3125], Training Loss: 0.567423403263092\n",
      "Epoch: [7/7], Step: [1401/3125], Validation Acc: 62.5\n",
      "Epoch: [7/7], Step: [1401/3125], Training Loss: 0.651828944683075\n",
      "Epoch: [7/7], Step: [1501/3125], Validation Acc: 61.7\n",
      "Epoch: [7/7], Step: [1501/3125], Training Loss: 0.4868805706501007\n",
      "Epoch: [7/7], Step: [1601/3125], Validation Acc: 60.7\n",
      "Epoch: [7/7], Step: [1601/3125], Training Loss: 0.5714244842529297\n",
      "Epoch: [7/7], Step: [1701/3125], Validation Acc: 60.7\n",
      "Epoch: [7/7], Step: [1701/3125], Training Loss: 0.6599637866020203\n",
      "Epoch: [7/7], Step: [1801/3125], Validation Acc: 61.9\n",
      "Epoch: [7/7], Step: [1801/3125], Training Loss: 0.7331032752990723\n",
      "Epoch: [7/7], Step: [1901/3125], Validation Acc: 60.8\n",
      "Epoch: [7/7], Step: [1901/3125], Training Loss: 0.4236988425254822\n",
      "Epoch: [7/7], Step: [2001/3125], Validation Acc: 60.9\n",
      "Epoch: [7/7], Step: [2001/3125], Training Loss: 0.6272286772727966\n",
      "Epoch: [7/7], Step: [2101/3125], Validation Acc: 64.3\n",
      "Epoch: [7/7], Step: [2101/3125], Training Loss: 0.5838388204574585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/7], Step: [2201/3125], Validation Acc: 62.7\n",
      "Epoch: [7/7], Step: [2201/3125], Training Loss: 0.5812860727310181\n",
      "Epoch: [7/7], Step: [2301/3125], Validation Acc: 61.5\n",
      "Epoch: [7/7], Step: [2301/3125], Training Loss: 0.8683007955551147\n",
      "Epoch: [7/7], Step: [2401/3125], Validation Acc: 61.6\n",
      "Epoch: [7/7], Step: [2401/3125], Training Loss: 0.7669524550437927\n",
      "Epoch: [7/7], Step: [2501/3125], Validation Acc: 62.0\n",
      "Epoch: [7/7], Step: [2501/3125], Training Loss: 0.4694633185863495\n",
      "Epoch: [7/7], Step: [2601/3125], Validation Acc: 62.0\n",
      "Epoch: [7/7], Step: [2601/3125], Training Loss: 0.5892811417579651\n",
      "Epoch: [7/7], Step: [2701/3125], Validation Acc: 61.4\n",
      "Epoch: [7/7], Step: [2701/3125], Training Loss: 0.6974173188209534\n",
      "Epoch: [7/7], Step: [2801/3125], Validation Acc: 63.2\n",
      "Epoch: [7/7], Step: [2801/3125], Training Loss: 0.698179304599762\n",
      "Epoch: [7/7], Step: [2901/3125], Validation Acc: 62.3\n",
      "Epoch: [7/7], Step: [2901/3125], Training Loss: 0.656996488571167\n",
      "Epoch: [7/7], Step: [3001/3125], Validation Acc: 63.9\n",
      "Epoch: [7/7], Step: [3001/3125], Training Loss: 0.7508187294006348\n",
      "Epoch: [7/7], Step: [3101/3125], Validation Acc: 62.8\n",
      "Epoch: [7/7], Step: [3101/3125], Training Loss: 0.6845691204071045\n",
      "Max Validation Accuracy: 64.4\n"
     ]
    }
   ],
   "source": [
    "model = CNN(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).cuda()\n",
    "max_val_acc, losses, xs, val_accs = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ea09570b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "plot_loss_and_validation_curves(losses, xs, val_accs, \"Standard CNN, element wise multiply, # params: {}\".format(get_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/home/vrajiv/rnn-cnn-natural-language-inference/best_cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard CNN** architecture and training hyperparameters: 2 Convolutional layers (each with kernel size 3) followed by 2 fully connected layers, hidden_size 300, 7 epochs, lr = 0.001. Used embedding layer with FastText vectors, with freeze = False. I've also included the results with freeze = True. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameters changed                  | CNN (val accuracy) | RNN (val accuracy) |\n",
    "|------------------------------------------|:------------------:|:------------------:|\n",
    "| Standard, freeze=False                   |        67.5%       |        69.9%       |\n",
    "| Standard, freeze=True                    |        67.2%       |        69.2%       |\n",
    "| Hidden size: 400                         |        67.2%       |        70.0%       |\n",
    "| Hidden size: 500                         |        67.1%       |        68.8%       |\n",
    "| Dropout (p = 0.5)                        |        66.6%       |        68.9%       |\n",
    "| Dropout (p = 0.2)                        |        67.2%       |        69.8%       |\n",
    "| Interaction: sum                         |        62.2%       |        64.1%       |\n",
    "| Interaction: element-wise multiplication |        65.8%       |        71.2%       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val = pd.read_csv('mnli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile for sentence length (in characters): 151.0\n"
     ]
    }
   ],
   "source": [
    "sentence_length_75 = pd.Series([len(x) for x in mnli_val['sentence1']]).describe()['75%']\n",
    "print(\"75th percentile for sentence length (in characters): {}\".format(sentence_length_75))\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary of (sent1, sent2, label) data, by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_dict = {}\n",
    "for x in mnli_val['genre'].unique():\n",
    "    filtered = mnli_val[mnli_val['genre'] == x]\n",
    "    mnli_val_dict[x] = {}\n",
    "    mnli_val_dict[x][\"sent1s\"] = list(filtered[\"sentence1\"])\n",
    "    mnli_val_dict[x][\"sent2s\"] = list(filtered[\"sentence2\"])\n",
    "    mnli_val_dict[x][\"label\"] = convert_labels_to_integers(list(filtered[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "[0 1 2]\n",
      "1005\n",
      "[0 1 2]\n",
      "1002\n",
      "[0 1 2]\n",
      "1016\n",
      "[0 1 2]\n",
      "982\n",
      "[0 1 2]\n",
      "France knew a good thing when she seized one , but then so did Britain .\n",
      "France knew this was a good place to stay .\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for x in mnli_val_dict.keys():\n",
    "    print(len(mnli_val_dict[x][\"sent1s\"]))\n",
    "    print(np.unique(mnli_val_dict[x][\"label\"]))\n",
    "    \n",
    "# quick verify\n",
    "verify_order(mnli_val_dict['travel'][\"sent1s\"], mnli_val_dict['travel'][\"sent2s\"], mnli_val_dict['travel'][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 45\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "No , do n't answer .\n",
      "Please respond .\n",
      "0\n",
      "Then I considered .\n",
      "Then , I thought if I should accepted to go with him .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 58\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "but how do you know the good from the bad\n",
      "Why care if it 's good or bad ?\n",
      "0\n",
      "so i guess my experience is is just with what we did and and so they did n't really go through the child care route they were able to be home together\n",
      "It was a good thing that they did n't go the child care route as they were able to be home with their child more often .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 92\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Indeed , 58 percent of Columbia/HCA 's beds lie empty , compared with 35 percent of nonprofit beds .\n",
      "58 % of Columbia/HCA 's beds are empty .\n",
      "1\n",
      "Enthusiasm for Disney 's Broadway production of The Lion King dwindles .\n",
      "The broadway production of The Lion King was amazing , but audiences are getting bored .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 43\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Mr. Erlenborn attended undergraduate courses at the University of Notre Dame , Indiana University , the University of Illinois , and Loyala University of Chicago .\n",
      "Mr. Erlenborn 's favorite university is located in Chicago .\n",
      "2\n",
      "STANDARD COSTING - A costing method that attaches costs to cost objects based on reasonable estimates or cost studies and by means of budgeted rates rather than according to actual costs incurred .\n",
      "Standard Costing was applied to the ledger .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Random seed for shuffling: 95\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "For centuries , the Loire river was a vital highway between the Atlantic and the heart of France .\n",
      "The Loire connected central France to the Atlantic .\n",
      "1\n",
      "Very few emperors were reluctant to submit to Fujiwara domination .\n",
      "Not many rulers had any hesitation in submitting to Fujiwara .\n",
      "1\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre fiction has validation accuracy: 42.311557788944725\n",
      "Genre telephone has validation accuracy: 43.582089552238806\n",
      "Genre slate has validation accuracy: 41.616766467065865\n",
      "Genre government has validation accuracy: 39.76377952755905\n",
      "Genre travel has validation accuracy: 42.56619144602851\n"
     ]
    }
   ],
   "source": [
    "# for each genre, build validation set and evaluate on it. \n",
    "cnn_results = {}\n",
    "model = CNN(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).cuda()\n",
    "model.load_state_dict(torch.load(\"/home/vrajiv/rnn-cnn-natural-language-inference/best_cnn\"))\n",
    "model.eval()\n",
    "for genre in mnli_val_dict.keys():\n",
    "    sent1_val_indices, sent2_val_indices, val_label = data_pipeline(mnli_val_dict[genre][\"sent1s\"], \n",
    "                                                                    mnli_val_dict[genre][\"sent2s\"], \n",
    "                                                                    mnli_val_dict[genre][\"label\"])\n",
    "    val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             collate_fn=twosentences_collate_func,\n",
    "                                             #shuffle=True\n",
    "                                             )\n",
    "    cnn_results[genre] = test_model(val_loader, model)\n",
    "\n",
    "for genre in mnli_val_dict.keys():\n",
    "    print(\"Genre {} has validation accuracy: {}\".format(genre, cnn_results[genre]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| genre      | CNN (val accuracy) | RNN (val accuracy) |\n",
    "|------------|--------------------|--------------------|\n",
    "| fiction    |       42.31%       |       45.53%       |\n",
    "| telephone  |       43.58%       |       40.40%       |\n",
    "| travel     |       42.57%       |       41.65%       |\n",
    "| slate      |       41.62%       |       40.52%       |\n",
    "| government |       39.76%       |       41.43%       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 correct and 3 incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1\n",
      "incorrect:  0\n",
      "correct:  2\n",
      "incorrect:  2\n",
      "incorrect:  1\n",
      "correct:  0\n"
     ]
    }
   ],
   "source": [
    "model = CNN(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).cuda()\n",
    "model.load_state_dict(torch.load(\"/home/vrajiv/rnn-cnn-natural-language-inference/best_cnn\"))\n",
    "model.eval()\n",
    "\n",
    "eval_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=1, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "\n",
    "i = 0\n",
    "incorrect_data = []\n",
    "correct_data = []\n",
    "corr_count = 0\n",
    "incorr_count = 0\n",
    "for data, sent1_lengths, sent2_lengths, labels in eval_loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.cuda(), sent1_lengths.cuda(), sent2_lengths.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        if (predicted.squeeze().item() == labels.squeeze().item() and corr_count <= 2):\n",
    "            corr_count += 1\n",
    "            correct_data.append(snli_val.iloc[[i]])\n",
    "            print(\"correct: \", labels.squeeze().item())\n",
    "        elif (predicted.squeeze().item() != labels.squeeze().item() and incorr_count <= 2):\n",
    "            incorr_count += 1\n",
    "            incorrect_data.append(snli_val.iloc[[i]])\n",
    "            print(\"incorrect: \", labels.squeeze().item())\n",
    "        i += 1\n",
    "        \n",
    "        if corr_count == 3 and incorr_count == 3:\n",
    "            break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Three women on a stage , one wearing red shoes...\n",
      "Name: sentence1, dtype: object\n",
      "0    There are two women standing on the stage\n",
      "Name: sentence2, dtype: object\n",
      "0    contradiction\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "2    bicycles stationed while a group of people soc...\n",
      "Name: sentence1, dtype: object\n",
      "2    People get together near a stand of bicycles .\n",
      "Name: sentence2, dtype: object\n",
      "2    entailment\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "6    Two men are listening to music through headpho...\n",
      "Name: sentence1, dtype: object\n",
      "6    Two men listen to music .\n",
      "Name: sentence2, dtype: object\n",
      "6    entailment\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(correct_data)):\n",
    "    print(correct_data[i][\"sentence1\"])\n",
    "    print(correct_data[i][\"sentence2\"])\n",
    "    print(correct_data[i][\"label\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    Four people sit on a subway two read books , o...\n",
      "Name: sentence1, dtype: object\n",
      "1    Multiple people are on a subway together , wit...\n",
      "Name: sentence2, dtype: object\n",
      "1    entailment\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "3    Man in overalls with two horses .\n",
      "Name: sentence1, dtype: object\n",
      "3    a man in overalls with two horses\n",
      "Name: sentence2, dtype: object\n",
      "3    entailment\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "4    Man observes a wavelength given off by an elec...\n",
      "Name: sentence1, dtype: object\n",
      "4    The man is examining what wavelength is given ...\n",
      "Name: sentence2, dtype: object\n",
      "4    entailment\n",
      "Name: label, dtype: object\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(incorrect_data)):\n",
    "    print(incorrect_data[i][\"sentence1\"])\n",
    "    print(incorrect_data[i][\"sentence2\"])\n",
    "    print(incorrect_data[i][\"label\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model seems to be good at associating numbers of objects to each other, either for contradiction or entailment purposes. It does not seem to effectively learn how sentences can be structured differently but give the same meaning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
