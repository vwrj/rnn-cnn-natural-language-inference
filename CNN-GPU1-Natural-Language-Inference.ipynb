{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for each step in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    print(n, d)\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "def build_vocab():\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    word_vectors = pkl.load(open(\"fasttext_word_vectors.p\", \"rb\"))\n",
    "    id2token = list(word_vectors.keys())\n",
    "    token2id = dict(zip(word_vectors, range(2,2+len(word_vectors)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return word_vectors, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integers(data_label):\n",
    "    for i in range(len(data_label)):\n",
    "        if data_label[i] == \"contradiction\":\n",
    "            data_label[i] = 0\n",
    "        elif data_label[i] == \"entailment\":\n",
    "            data_label[i] = 1\n",
    "        elif data_label[i] == \"neutral\":\n",
    "            data_label[i] = 2\n",
    "    return data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_order(sent1_data, sent2_data, data_label):\n",
    "    i = random.randint(1, len(sent1_data))\n",
    "    print(sent1_data[i])\n",
    "    print(sent2_data[i])\n",
    "    print(data_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize each entry in a list of sentences\n",
    "def tokenize(sentence_list):\n",
    "    return [word_tokenize(sentence_list[i]) for i in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"one-hot encode\": convert each token to id in vocabulary vector (token2id)\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary & embedding matrix from FastText vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights = np.array(list(word_vectors.values()))\n",
    "pad_vec = np.zeros((1, 300))\n",
    "unk_vec = np.random.randn(1, 300) * 0.01\n",
    "pad_unk_vecs = np.vstack((pad_vec, unk_vec))\n",
    "_WEIGHTS = np.vstack((pad_unk_vecs, _weights))\n",
    "_WEIGHTS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pre-process data for TwoSentenceModel\n",
    "#### Shuffle, word tokenize, one-hot index into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(sent1s, sent2s, labels, verify=True):\n",
    "    labels = convert_labels_to_integers(labels)\n",
    "    seed = random.randint(1, 100)\n",
    "    print(\"Random seed for shuffling: {}\".format(seed))\n",
    "    random.Random(seed).shuffle(sent1s)\n",
    "    random.Random(seed).shuffle(sent2s)\n",
    "    random.Random(seed).shuffle(labels)\n",
    "    \n",
    "    print(\"\\nVerifying that the data and label match after shuffling\")\n",
    "    if verify:\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "        verify_order(sent1s, sent2s, labels)\n",
    "          \n",
    "    print(\"\\nTokenizing sentence 1 list...\")    \n",
    "    sent1s_tokenized = tokenize(sent1s)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nTokenizing sentence 2 list... \")  \n",
    "    sent2s_tokenized = tokenize(sent2s)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    print(\"\\nOne-hot encoding words for sentence 1 list...\")  \n",
    "    sent1s_indices = token2index_dataset(sent1s_tokenized)\n",
    "    print(\"done!\")\n",
    "    print(\"\\nOne-hot encoding words for sentence 2 list...\")  \n",
    "    sent2s_indices = token2index_dataset(sent2s_tokenized)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return (sent1s_indices, sent2s_indices, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoSentencesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_data_list, sent2_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param sent1_data_list: list of sentence1's (index matches sentence2's and target_list below)\n",
    "        @param sent2_data_list: list of sentence2's\n",
    "        @param target_list: list of correct labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_data_list = sent1_data_list\n",
    "        self.sent2_data_list = sent2_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data_list) == len(self.target_list) and len(self.sent2_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[sentence, 1, tokens], [sentence, 2, tokens]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1_tokens_idx = self.sent1_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_tokens_idx = self.sent2_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        combined_tokens_idx = [sent1_tokens_idx, sent2_tokens_idx]\n",
    "        label = self.target_list[key]\n",
    "        return [combined_tokens_idx, len(sent1_tokens_idx), len(sent2_tokens_idx), label]\n",
    "\n",
    "def twosentences_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_length_list = []\n",
    "    label_list = []\n",
    "    combined_data_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[1])\n",
    "        sent2_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0][0]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[0][1]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        combined_data_list.append([padded_vec_1, padded_vec_2])\n",
    "    return [torch.from_numpy(np.array(combined_data_list)), \n",
    "            torch.LongTensor(sent1_length_list), torch.LongTensor(sent2_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 100000\n"
     ]
    }
   ],
   "source": [
    "snli_train = pd.read_csv('snli_train.tsv', sep='\\t')\n",
    "TRAIN_SIZE = 100000\n",
    "\n",
    "sent1_data = list(snli_train[\"sentence1\"])[:TRAIN_SIZE]\n",
    "sent2_data = list(snli_train[\"sentence2\"])[:TRAIN_SIZE]\n",
    "data_label = list(snli_train[\"label\"])[:TRAIN_SIZE]\n",
    "print(\"Size of training data: {}\".format(len(sent1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 92\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "This woman is smiling and talking on the phone while sitting on a stone wall .\n",
      "A woman sits in her garden on her cell phone .\n",
      "2\n",
      "Two young boys playing indoor soccer , one wears blue and white , the other wears red .\n",
      "There are children playing soccer\n",
      "1\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Finished creating train_loader.\n"
     ]
    }
   ],
   "source": [
    "sent1_train_indices, sent2_train_indices, train_label = data_pipeline(sent1_data, sent2_data, data_label)\n",
    "train_dataset = TwoSentencesDataset(sent1_train_indices, sent2_train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "print(\"Finished creating train_loader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of val data: 1000\n"
     ]
    }
   ],
   "source": [
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t')\n",
    "sent1_val = list(snli_val[\"sentence1\"])\n",
    "sent2_val = list(snli_val[\"sentence2\"])\n",
    "val_label = list(snli_val[\"label\"])\n",
    "print(\"Size of val data: {}\".format(len(sent1_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 18\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Two competitors in the last leg of a race , strong legs , long strides to the end .\n",
      "The crowd is cheering them on .\n",
      "2\n",
      "People in a warehouse , with one man kicking a bag .\n",
      "A group of dogs chasing a cat .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sent1_val_indices, sent2_val_indices, val_label = data_pipeline(sent1_val, sent2_val, val_label)\n",
    "val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=twosentences_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in snli_train['sentence1']]).describe()['75%']\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(_WEIGHTS))\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(30)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        seq_len = x.size()[2]\n",
    "        \n",
    "        sent1s = torch.tensor(x[:, 0, :]).cuda()\n",
    "        sent2s = torch.tensor(x[:, 1, :]).cuda()\n",
    "        ordered_sents = torch.cat([sent1s, sent2s], dim=0).cuda()\n",
    "\n",
    "        embed = self.embedding(ordered_sents)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = self.maxpool(hidden.transpose(1, 2)).transpose(1, 2).squeeze(dim=1)\n",
    "        \n",
    "        hidden_sent1s = hidden[0:batch_size, :]\n",
    "        hidden_sent2s = hidden[batch_size:, :]     \n",
    "        \n",
    "        linear1 = self.linear1(torch.cat([hidden_sent1s, hidden_sent2s], dim=1))\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s) + torch.tensor(hidden_sent2s))\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s)*torch.tensor(hidden_sent2s))\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)\n",
    "#         linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5948, -0.0249,  2.7304,  0.6314],\n",
      "        [-0.4873, -0.2135,  0.9251, -0.1956],\n",
      "        [ 0.1997,  0.3483,  0.0141, -1.7675]])\n",
      "tensor([[-0.0806,  2.2131, -0.6601, -0.7191],\n",
      "        [-1.5663, -0.6721, -0.4429,  0.6105],\n",
      "        [-0.8467,  0.0843, -2.0210,  0.8280]])\n",
      "tensor([[ 0.0479, -0.0551, -1.8023, -0.4540],\n",
      "        [ 0.7632,  0.1435, -0.4097, -0.1194],\n",
      "        [-0.1690,  0.0293, -0.0285, -1.4635]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, sent1_lengths, sent2_lengths, labels) in loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.cuda(), sent1_lengths.cuda(), sent2_lengths.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels.cuda()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 7, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005) \n",
    "    max_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, sent1_length_batch, sent2_length_batch, label_batch = data.cuda(), sent1_lengths.cuda(), sent2_lengths.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, sent1_length_batch, sent2_length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "                \n",
    "    print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/7], Step: [101/3125], Validation Acc: 40.2\n",
      "Epoch: [1/7], Step: [101/3125], Training Loss: 1.0947794914245605\n",
      "Epoch: [1/7], Step: [201/3125], Validation Acc: 44.6\n",
      "Epoch: [1/7], Step: [201/3125], Training Loss: 1.0399408340454102\n",
      "Epoch: [1/7], Step: [301/3125], Validation Acc: 45.1\n",
      "Epoch: [1/7], Step: [301/3125], Training Loss: 1.082045078277588\n",
      "Epoch: [1/7], Step: [401/3125], Validation Acc: 45.5\n",
      "Epoch: [1/7], Step: [401/3125], Training Loss: 1.0328264236450195\n",
      "Epoch: [1/7], Step: [501/3125], Validation Acc: 47.5\n",
      "Epoch: [1/7], Step: [501/3125], Training Loss: 1.1084997653961182\n",
      "Epoch: [1/7], Step: [601/3125], Validation Acc: 45.9\n",
      "Epoch: [1/7], Step: [601/3125], Training Loss: 0.9500107765197754\n",
      "Epoch: [1/7], Step: [701/3125], Validation Acc: 46.6\n",
      "Epoch: [1/7], Step: [701/3125], Training Loss: 1.0810883045196533\n",
      "Epoch: [1/7], Step: [801/3125], Validation Acc: 49.0\n",
      "Epoch: [1/7], Step: [801/3125], Training Loss: 1.0023455619812012\n",
      "Epoch: [1/7], Step: [901/3125], Validation Acc: 46.5\n",
      "Epoch: [1/7], Step: [901/3125], Training Loss: 1.0663899183273315\n",
      "Epoch: [1/7], Step: [1001/3125], Validation Acc: 47.8\n",
      "Epoch: [1/7], Step: [1001/3125], Training Loss: 0.9780768156051636\n",
      "Epoch: [1/7], Step: [1101/3125], Validation Acc: 48.4\n",
      "Epoch: [1/7], Step: [1101/3125], Training Loss: 1.0641316175460815\n",
      "Epoch: [1/7], Step: [1201/3125], Validation Acc: 50.2\n",
      "Epoch: [1/7], Step: [1201/3125], Training Loss: 0.8614610433578491\n",
      "Epoch: [1/7], Step: [1301/3125], Validation Acc: 48.9\n",
      "Epoch: [1/7], Step: [1301/3125], Training Loss: 1.1071468591690063\n",
      "Epoch: [1/7], Step: [1401/3125], Validation Acc: 50.1\n",
      "Epoch: [1/7], Step: [1401/3125], Training Loss: 0.9680373668670654\n",
      "Epoch: [1/7], Step: [1501/3125], Validation Acc: 50.0\n",
      "Epoch: [1/7], Step: [1501/3125], Training Loss: 1.074944019317627\n",
      "Epoch: [1/7], Step: [1601/3125], Validation Acc: 48.8\n",
      "Epoch: [1/7], Step: [1601/3125], Training Loss: 1.1755189895629883\n",
      "Epoch: [1/7], Step: [1701/3125], Validation Acc: 50.1\n",
      "Epoch: [1/7], Step: [1701/3125], Training Loss: 0.9034759998321533\n",
      "Epoch: [1/7], Step: [1801/3125], Validation Acc: 51.2\n",
      "Epoch: [1/7], Step: [1801/3125], Training Loss: 0.8891558647155762\n",
      "Epoch: [1/7], Step: [1901/3125], Validation Acc: 52.2\n",
      "Epoch: [1/7], Step: [1901/3125], Training Loss: 0.9306811094284058\n",
      "Epoch: [1/7], Step: [2001/3125], Validation Acc: 55.4\n",
      "Epoch: [1/7], Step: [2001/3125], Training Loss: 1.079498052597046\n",
      "Epoch: [1/7], Step: [2101/3125], Validation Acc: 52.6\n",
      "Epoch: [1/7], Step: [2101/3125], Training Loss: 1.0937776565551758\n",
      "Epoch: [1/7], Step: [2201/3125], Validation Acc: 53.9\n",
      "Epoch: [1/7], Step: [2201/3125], Training Loss: 0.8880749940872192\n",
      "Epoch: [1/7], Step: [2301/3125], Validation Acc: 55.0\n",
      "Epoch: [1/7], Step: [2301/3125], Training Loss: 0.9502215385437012\n",
      "Epoch: [1/7], Step: [2401/3125], Validation Acc: 56.9\n",
      "Epoch: [1/7], Step: [2401/3125], Training Loss: 0.9248161315917969\n",
      "Epoch: [1/7], Step: [2501/3125], Validation Acc: 58.1\n",
      "Epoch: [1/7], Step: [2501/3125], Training Loss: 0.9692712426185608\n",
      "Epoch: [1/7], Step: [2601/3125], Validation Acc: 58.4\n",
      "Epoch: [1/7], Step: [2601/3125], Training Loss: 0.875038206577301\n",
      "Epoch: [1/7], Step: [2701/3125], Validation Acc: 58.6\n",
      "Epoch: [1/7], Step: [2701/3125], Training Loss: 0.9161210656166077\n",
      "Epoch: [1/7], Step: [2801/3125], Validation Acc: 58.1\n",
      "Epoch: [1/7], Step: [2801/3125], Training Loss: 0.9264354109764099\n",
      "Epoch: [1/7], Step: [2901/3125], Validation Acc: 59.6\n",
      "Epoch: [1/7], Step: [2901/3125], Training Loss: 0.9130440354347229\n",
      "Epoch: [1/7], Step: [3001/3125], Validation Acc: 59.6\n",
      "Epoch: [1/7], Step: [3001/3125], Training Loss: 0.9747412800788879\n",
      "Epoch: [1/7], Step: [3101/3125], Validation Acc: 58.6\n",
      "Epoch: [1/7], Step: [3101/3125], Training Loss: 0.6084176898002625\n",
      "Epoch: [2/7], Step: [101/3125], Validation Acc: 59.9\n",
      "Epoch: [2/7], Step: [101/3125], Training Loss: 0.7590364813804626\n",
      "Epoch: [2/7], Step: [201/3125], Validation Acc: 59.6\n",
      "Epoch: [2/7], Step: [201/3125], Training Loss: 0.6504122018814087\n",
      "Epoch: [2/7], Step: [301/3125], Validation Acc: 60.2\n",
      "Epoch: [2/7], Step: [301/3125], Training Loss: 1.0603163242340088\n",
      "Epoch: [2/7], Step: [401/3125], Validation Acc: 59.4\n",
      "Epoch: [2/7], Step: [401/3125], Training Loss: 0.9109442234039307\n",
      "Epoch: [2/7], Step: [501/3125], Validation Acc: 60.2\n",
      "Epoch: [2/7], Step: [501/3125], Training Loss: 1.0529674291610718\n",
      "Epoch: [2/7], Step: [601/3125], Validation Acc: 60.8\n",
      "Epoch: [2/7], Step: [601/3125], Training Loss: 0.8165849447250366\n",
      "Epoch: [2/7], Step: [701/3125], Validation Acc: 59.0\n",
      "Epoch: [2/7], Step: [701/3125], Training Loss: 0.8325846195220947\n",
      "Epoch: [2/7], Step: [801/3125], Validation Acc: 59.1\n",
      "Epoch: [2/7], Step: [801/3125], Training Loss: 0.7958093881607056\n",
      "Epoch: [2/7], Step: [901/3125], Validation Acc: 60.1\n",
      "Epoch: [2/7], Step: [901/3125], Training Loss: 0.8556154370307922\n",
      "Epoch: [2/7], Step: [1001/3125], Validation Acc: 61.3\n",
      "Epoch: [2/7], Step: [1001/3125], Training Loss: 0.7553465366363525\n",
      "Epoch: [2/7], Step: [1101/3125], Validation Acc: 61.1\n",
      "Epoch: [2/7], Step: [1101/3125], Training Loss: 0.9901276230812073\n",
      "Epoch: [2/7], Step: [1201/3125], Validation Acc: 60.6\n",
      "Epoch: [2/7], Step: [1201/3125], Training Loss: 0.8296380043029785\n",
      "Epoch: [2/7], Step: [1301/3125], Validation Acc: 60.5\n",
      "Epoch: [2/7], Step: [1301/3125], Training Loss: 0.9753706455230713\n",
      "Epoch: [2/7], Step: [1401/3125], Validation Acc: 61.5\n",
      "Epoch: [2/7], Step: [1401/3125], Training Loss: 0.845875084400177\n",
      "Epoch: [2/7], Step: [1501/3125], Validation Acc: 62.5\n",
      "Epoch: [2/7], Step: [1501/3125], Training Loss: 0.9671884775161743\n",
      "Epoch: [2/7], Step: [1601/3125], Validation Acc: 60.1\n",
      "Epoch: [2/7], Step: [1601/3125], Training Loss: 1.0838111639022827\n",
      "Epoch: [2/7], Step: [1701/3125], Validation Acc: 58.7\n",
      "Epoch: [2/7], Step: [1701/3125], Training Loss: 0.8015663027763367\n",
      "Epoch: [2/7], Step: [1801/3125], Validation Acc: 58.3\n",
      "Epoch: [2/7], Step: [1801/3125], Training Loss: 0.8611451387405396\n",
      "Epoch: [2/7], Step: [1901/3125], Validation Acc: 60.0\n",
      "Epoch: [2/7], Step: [1901/3125], Training Loss: 0.9057139158248901\n",
      "Epoch: [2/7], Step: [2001/3125], Validation Acc: 61.5\n",
      "Epoch: [2/7], Step: [2001/3125], Training Loss: 0.9071311950683594\n",
      "Epoch: [2/7], Step: [2101/3125], Validation Acc: 58.4\n",
      "Epoch: [2/7], Step: [2101/3125], Training Loss: 0.9662080407142639\n",
      "Epoch: [2/7], Step: [2201/3125], Validation Acc: 58.6\n",
      "Epoch: [2/7], Step: [2201/3125], Training Loss: 0.748220682144165\n",
      "Epoch: [2/7], Step: [2301/3125], Validation Acc: 59.1\n",
      "Epoch: [2/7], Step: [2301/3125], Training Loss: 0.9392326474189758\n",
      "Epoch: [2/7], Step: [2401/3125], Validation Acc: 61.1\n",
      "Epoch: [2/7], Step: [2401/3125], Training Loss: 0.906093955039978\n",
      "Epoch: [2/7], Step: [2501/3125], Validation Acc: 61.2\n",
      "Epoch: [2/7], Step: [2501/3125], Training Loss: 0.8864562511444092\n",
      "Epoch: [2/7], Step: [2601/3125], Validation Acc: 60.6\n",
      "Epoch: [2/7], Step: [2601/3125], Training Loss: 0.8417396545410156\n",
      "Epoch: [2/7], Step: [2701/3125], Validation Acc: 62.5\n",
      "Epoch: [2/7], Step: [2701/3125], Training Loss: 0.8438541889190674\n",
      "Epoch: [2/7], Step: [2801/3125], Validation Acc: 60.0\n",
      "Epoch: [2/7], Step: [2801/3125], Training Loss: 0.8091670870780945\n",
      "Epoch: [2/7], Step: [2901/3125], Validation Acc: 61.9\n",
      "Epoch: [2/7], Step: [2901/3125], Training Loss: 0.8478303551673889\n",
      "Epoch: [2/7], Step: [3001/3125], Validation Acc: 60.6\n",
      "Epoch: [2/7], Step: [3001/3125], Training Loss: 0.8867359757423401\n",
      "Epoch: [2/7], Step: [3101/3125], Validation Acc: 60.1\n",
      "Epoch: [2/7], Step: [3101/3125], Training Loss: 0.5308968424797058\n",
      "Epoch: [3/7], Step: [101/3125], Validation Acc: 60.4\n",
      "Epoch: [3/7], Step: [101/3125], Training Loss: 0.6794521808624268\n",
      "Epoch: [3/7], Step: [201/3125], Validation Acc: 62.2\n",
      "Epoch: [3/7], Step: [201/3125], Training Loss: 0.6133549213409424\n",
      "Epoch: [3/7], Step: [301/3125], Validation Acc: 61.2\n",
      "Epoch: [3/7], Step: [301/3125], Training Loss: 0.991862952709198\n",
      "Epoch: [3/7], Step: [401/3125], Validation Acc: 61.5\n",
      "Epoch: [3/7], Step: [401/3125], Training Loss: 0.8101140260696411\n",
      "Epoch: [3/7], Step: [501/3125], Validation Acc: 60.7\n",
      "Epoch: [3/7], Step: [501/3125], Training Loss: 1.042901635169983\n",
      "Epoch: [3/7], Step: [601/3125], Validation Acc: 59.5\n",
      "Epoch: [3/7], Step: [601/3125], Training Loss: 0.7675953507423401\n",
      "Epoch: [3/7], Step: [701/3125], Validation Acc: 60.2\n",
      "Epoch: [3/7], Step: [701/3125], Training Loss: 0.7615829706192017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/7], Step: [801/3125], Validation Acc: 60.3\n",
      "Epoch: [3/7], Step: [801/3125], Training Loss: 0.7389848828315735\n",
      "Epoch: [3/7], Step: [901/3125], Validation Acc: 61.7\n",
      "Epoch: [3/7], Step: [901/3125], Training Loss: 0.8025896549224854\n",
      "Epoch: [3/7], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [3/7], Step: [1001/3125], Training Loss: 0.6803643107414246\n",
      "Epoch: [3/7], Step: [1101/3125], Validation Acc: 61.3\n",
      "Epoch: [3/7], Step: [1101/3125], Training Loss: 0.9766620397567749\n",
      "Epoch: [3/7], Step: [1201/3125], Validation Acc: 62.2\n",
      "Epoch: [3/7], Step: [1201/3125], Training Loss: 0.7305264472961426\n",
      "Epoch: [3/7], Step: [1301/3125], Validation Acc: 61.2\n",
      "Epoch: [3/7], Step: [1301/3125], Training Loss: 0.997424304485321\n",
      "Epoch: [3/7], Step: [1401/3125], Validation Acc: 62.0\n",
      "Epoch: [3/7], Step: [1401/3125], Training Loss: 0.8037538528442383\n",
      "Epoch: [3/7], Step: [1501/3125], Validation Acc: 63.8\n",
      "Epoch: [3/7], Step: [1501/3125], Training Loss: 0.9154831171035767\n",
      "Epoch: [3/7], Step: [1601/3125], Validation Acc: 60.9\n",
      "Epoch: [3/7], Step: [1601/3125], Training Loss: 1.016065001487732\n",
      "Epoch: [3/7], Step: [1701/3125], Validation Acc: 61.8\n",
      "Epoch: [3/7], Step: [1701/3125], Training Loss: 0.7426674962043762\n",
      "Epoch: [3/7], Step: [1801/3125], Validation Acc: 60.6\n",
      "Epoch: [3/7], Step: [1801/3125], Training Loss: 0.8138453960418701\n",
      "Epoch: [3/7], Step: [1901/3125], Validation Acc: 61.0\n",
      "Epoch: [3/7], Step: [1901/3125], Training Loss: 0.8866025805473328\n",
      "Epoch: [3/7], Step: [2001/3125], Validation Acc: 62.0\n",
      "Epoch: [3/7], Step: [2001/3125], Training Loss: 0.8672987818717957\n",
      "Epoch: [3/7], Step: [2101/3125], Validation Acc: 60.6\n",
      "Epoch: [3/7], Step: [2101/3125], Training Loss: 0.9257500171661377\n",
      "Epoch: [3/7], Step: [2201/3125], Validation Acc: 59.9\n",
      "Epoch: [3/7], Step: [2201/3125], Training Loss: 0.7469951510429382\n",
      "Epoch: [3/7], Step: [2301/3125], Validation Acc: 60.4\n",
      "Epoch: [3/7], Step: [2301/3125], Training Loss: 0.8769674897193909\n",
      "Epoch: [3/7], Step: [2401/3125], Validation Acc: 61.1\n",
      "Epoch: [3/7], Step: [2401/3125], Training Loss: 0.8971760869026184\n",
      "Epoch: [3/7], Step: [2501/3125], Validation Acc: 61.5\n",
      "Epoch: [3/7], Step: [2501/3125], Training Loss: 0.8777896165847778\n",
      "Epoch: [3/7], Step: [2601/3125], Validation Acc: 61.6\n",
      "Epoch: [3/7], Step: [2601/3125], Training Loss: 0.8274368047714233\n",
      "Epoch: [3/7], Step: [2701/3125], Validation Acc: 63.3\n",
      "Epoch: [3/7], Step: [2701/3125], Training Loss: 0.7781006693840027\n",
      "Epoch: [3/7], Step: [2801/3125], Validation Acc: 61.2\n",
      "Epoch: [3/7], Step: [2801/3125], Training Loss: 0.7937500476837158\n",
      "Epoch: [3/7], Step: [2901/3125], Validation Acc: 62.0\n",
      "Epoch: [3/7], Step: [2901/3125], Training Loss: 0.811075747013092\n",
      "Epoch: [3/7], Step: [3001/3125], Validation Acc: 62.9\n",
      "Epoch: [3/7], Step: [3001/3125], Training Loss: 0.8424727916717529\n",
      "Epoch: [3/7], Step: [3101/3125], Validation Acc: 62.5\n",
      "Epoch: [3/7], Step: [3101/3125], Training Loss: 0.48392075300216675\n",
      "Epoch: [4/7], Step: [101/3125], Validation Acc: 61.4\n",
      "Epoch: [4/7], Step: [101/3125], Training Loss: 0.6247244477272034\n",
      "Epoch: [4/7], Step: [201/3125], Validation Acc: 62.1\n",
      "Epoch: [4/7], Step: [201/3125], Training Loss: 0.6280914545059204\n",
      "Epoch: [4/7], Step: [301/3125], Validation Acc: 62.3\n",
      "Epoch: [4/7], Step: [301/3125], Training Loss: 0.8856221437454224\n",
      "Epoch: [4/7], Step: [401/3125], Validation Acc: 62.1\n",
      "Epoch: [4/7], Step: [401/3125], Training Loss: 0.7862541675567627\n",
      "Epoch: [4/7], Step: [501/3125], Validation Acc: 63.3\n",
      "Epoch: [4/7], Step: [501/3125], Training Loss: 1.0210777521133423\n",
      "Epoch: [4/7], Step: [601/3125], Validation Acc: 63.1\n",
      "Epoch: [4/7], Step: [601/3125], Training Loss: 0.7110145688056946\n",
      "Epoch: [4/7], Step: [701/3125], Validation Acc: 61.4\n",
      "Epoch: [4/7], Step: [701/3125], Training Loss: 0.7110528945922852\n",
      "Epoch: [4/7], Step: [801/3125], Validation Acc: 61.9\n",
      "Epoch: [4/7], Step: [801/3125], Training Loss: 0.7158461213111877\n",
      "Epoch: [4/7], Step: [901/3125], Validation Acc: 61.1\n",
      "Epoch: [4/7], Step: [901/3125], Training Loss: 0.8117480278015137\n",
      "Epoch: [4/7], Step: [1001/3125], Validation Acc: 63.2\n",
      "Epoch: [4/7], Step: [1001/3125], Training Loss: 0.671807587146759\n",
      "Epoch: [4/7], Step: [1101/3125], Validation Acc: 62.2\n",
      "Epoch: [4/7], Step: [1101/3125], Training Loss: 0.9819719791412354\n",
      "Epoch: [4/7], Step: [1201/3125], Validation Acc: 63.4\n",
      "Epoch: [4/7], Step: [1201/3125], Training Loss: 0.6863443851470947\n",
      "Epoch: [4/7], Step: [1301/3125], Validation Acc: 62.0\n",
      "Epoch: [4/7], Step: [1301/3125], Training Loss: 0.9786074757575989\n",
      "Epoch: [4/7], Step: [1401/3125], Validation Acc: 62.3\n",
      "Epoch: [4/7], Step: [1401/3125], Training Loss: 0.7526190876960754\n",
      "Epoch: [4/7], Step: [1501/3125], Validation Acc: 64.6\n",
      "Epoch: [4/7], Step: [1501/3125], Training Loss: 0.9040618538856506\n",
      "Epoch: [4/7], Step: [1601/3125], Validation Acc: 61.8\n",
      "Epoch: [4/7], Step: [1601/3125], Training Loss: 0.9378232359886169\n",
      "Epoch: [4/7], Step: [1701/3125], Validation Acc: 62.7\n",
      "Epoch: [4/7], Step: [1701/3125], Training Loss: 0.7166574597358704\n",
      "Epoch: [4/7], Step: [1801/3125], Validation Acc: 63.9\n",
      "Epoch: [4/7], Step: [1801/3125], Training Loss: 0.7269847393035889\n",
      "Epoch: [4/7], Step: [1901/3125], Validation Acc: 61.7\n",
      "Epoch: [4/7], Step: [1901/3125], Training Loss: 0.8593963384628296\n",
      "Epoch: [4/7], Step: [2001/3125], Validation Acc: 62.5\n",
      "Epoch: [4/7], Step: [2001/3125], Training Loss: 0.8858942985534668\n",
      "Epoch: [4/7], Step: [2101/3125], Validation Acc: 61.6\n",
      "Epoch: [4/7], Step: [2101/3125], Training Loss: 0.8975319266319275\n",
      "Epoch: [4/7], Step: [2201/3125], Validation Acc: 62.6\n",
      "Epoch: [4/7], Step: [2201/3125], Training Loss: 0.73480623960495\n",
      "Epoch: [4/7], Step: [2301/3125], Validation Acc: 61.5\n",
      "Epoch: [4/7], Step: [2301/3125], Training Loss: 0.8633330464363098\n",
      "Epoch: [4/7], Step: [2401/3125], Validation Acc: 62.0\n",
      "Epoch: [4/7], Step: [2401/3125], Training Loss: 0.8842419385910034\n",
      "Epoch: [4/7], Step: [2501/3125], Validation Acc: 63.2\n",
      "Epoch: [4/7], Step: [2501/3125], Training Loss: 0.8652943968772888\n",
      "Epoch: [4/7], Step: [2601/3125], Validation Acc: 63.0\n",
      "Epoch: [4/7], Step: [2601/3125], Training Loss: 0.8278960585594177\n",
      "Epoch: [4/7], Step: [2701/3125], Validation Acc: 64.6\n",
      "Epoch: [4/7], Step: [2701/3125], Training Loss: 0.755253791809082\n",
      "Epoch: [4/7], Step: [2801/3125], Validation Acc: 63.5\n",
      "Epoch: [4/7], Step: [2801/3125], Training Loss: 0.7795224189758301\n",
      "Epoch: [4/7], Step: [2901/3125], Validation Acc: 62.8\n",
      "Epoch: [4/7], Step: [2901/3125], Training Loss: 0.7852985858917236\n",
      "Epoch: [4/7], Step: [3001/3125], Validation Acc: 63.0\n",
      "Epoch: [4/7], Step: [3001/3125], Training Loss: 0.8663405179977417\n",
      "Epoch: [4/7], Step: [3101/3125], Validation Acc: 63.5\n",
      "Epoch: [4/7], Step: [3101/3125], Training Loss: 0.4940653443336487\n",
      "Epoch: [5/7], Step: [101/3125], Validation Acc: 62.1\n",
      "Epoch: [5/7], Step: [101/3125], Training Loss: 0.6132931113243103\n",
      "Epoch: [5/7], Step: [201/3125], Validation Acc: 63.1\n",
      "Epoch: [5/7], Step: [201/3125], Training Loss: 0.6243047118186951\n",
      "Epoch: [5/7], Step: [301/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [301/3125], Training Loss: 0.8490651249885559\n",
      "Epoch: [5/7], Step: [401/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [401/3125], Training Loss: 0.7748721837997437\n",
      "Epoch: [5/7], Step: [501/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [501/3125], Training Loss: 0.9956482648849487\n",
      "Epoch: [5/7], Step: [601/3125], Validation Acc: 63.8\n",
      "Epoch: [5/7], Step: [601/3125], Training Loss: 0.7102380990982056\n",
      "Epoch: [5/7], Step: [701/3125], Validation Acc: 62.9\n",
      "Epoch: [5/7], Step: [701/3125], Training Loss: 0.7449935674667358\n",
      "Epoch: [5/7], Step: [801/3125], Validation Acc: 61.9\n",
      "Epoch: [5/7], Step: [801/3125], Training Loss: 0.7214140892028809\n",
      "Epoch: [5/7], Step: [901/3125], Validation Acc: 62.0\n",
      "Epoch: [5/7], Step: [901/3125], Training Loss: 0.8019004464149475\n",
      "Epoch: [5/7], Step: [1001/3125], Validation Acc: 63.2\n",
      "Epoch: [5/7], Step: [1001/3125], Training Loss: 0.6352968215942383\n",
      "Epoch: [5/7], Step: [1101/3125], Validation Acc: 63.6\n",
      "Epoch: [5/7], Step: [1101/3125], Training Loss: 0.963026762008667\n",
      "Epoch: [5/7], Step: [1201/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [1201/3125], Training Loss: 0.6713165044784546\n",
      "Epoch: [5/7], Step: [1301/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [1301/3125], Training Loss: 0.9877426028251648\n",
      "Epoch: [5/7], Step: [1401/3125], Validation Acc: 64.6\n",
      "Epoch: [5/7], Step: [1401/3125], Training Loss: 0.7639433145523071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/7], Step: [1501/3125], Validation Acc: 63.8\n",
      "Epoch: [5/7], Step: [1501/3125], Training Loss: 0.8840357661247253\n",
      "Epoch: [5/7], Step: [1601/3125], Validation Acc: 62.3\n",
      "Epoch: [5/7], Step: [1601/3125], Training Loss: 0.9186645746231079\n",
      "Epoch: [5/7], Step: [1701/3125], Validation Acc: 62.7\n",
      "Epoch: [5/7], Step: [1701/3125], Training Loss: 0.6991443037986755\n",
      "Epoch: [5/7], Step: [1801/3125], Validation Acc: 63.3\n",
      "Epoch: [5/7], Step: [1801/3125], Training Loss: 0.7221181988716125\n",
      "Epoch: [5/7], Step: [1901/3125], Validation Acc: 62.6\n",
      "Epoch: [5/7], Step: [1901/3125], Training Loss: 0.8530296087265015\n",
      "Epoch: [5/7], Step: [2001/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [2001/3125], Training Loss: 0.9028189778327942\n",
      "Epoch: [5/7], Step: [2101/3125], Validation Acc: 62.4\n",
      "Epoch: [5/7], Step: [2101/3125], Training Loss: 0.8723999857902527\n",
      "Epoch: [5/7], Step: [2201/3125], Validation Acc: 62.2\n",
      "Epoch: [5/7], Step: [2201/3125], Training Loss: 0.7387920022010803\n",
      "Epoch: [5/7], Step: [2301/3125], Validation Acc: 62.5\n",
      "Epoch: [5/7], Step: [2301/3125], Training Loss: 0.8046462535858154\n",
      "Epoch: [5/7], Step: [2401/3125], Validation Acc: 62.8\n",
      "Epoch: [5/7], Step: [2401/3125], Training Loss: 0.8738080263137817\n",
      "Epoch: [5/7], Step: [2501/3125], Validation Acc: 63.5\n",
      "Epoch: [5/7], Step: [2501/3125], Training Loss: 0.8632042407989502\n",
      "Epoch: [5/7], Step: [2601/3125], Validation Acc: 63.0\n",
      "Epoch: [5/7], Step: [2601/3125], Training Loss: 0.7975590229034424\n",
      "Epoch: [5/7], Step: [2701/3125], Validation Acc: 64.3\n",
      "Epoch: [5/7], Step: [2701/3125], Training Loss: 0.7468588352203369\n",
      "Epoch: [5/7], Step: [2801/3125], Validation Acc: 63.8\n",
      "Epoch: [5/7], Step: [2801/3125], Training Loss: 0.7909831404685974\n",
      "Epoch: [5/7], Step: [2901/3125], Validation Acc: 63.6\n",
      "Epoch: [5/7], Step: [2901/3125], Training Loss: 0.7689744234085083\n",
      "Epoch: [5/7], Step: [3001/3125], Validation Acc: 64.1\n",
      "Epoch: [5/7], Step: [3001/3125], Training Loss: 0.8512436747550964\n",
      "Epoch: [5/7], Step: [3101/3125], Validation Acc: 64.7\n",
      "Epoch: [5/7], Step: [3101/3125], Training Loss: 0.5033524632453918\n",
      "Epoch: [6/7], Step: [101/3125], Validation Acc: 63.2\n",
      "Epoch: [6/7], Step: [101/3125], Training Loss: 0.6013636589050293\n",
      "Epoch: [6/7], Step: [201/3125], Validation Acc: 65.0\n",
      "Epoch: [6/7], Step: [201/3125], Training Loss: 0.5883010625839233\n",
      "Epoch: [6/7], Step: [301/3125], Validation Acc: 62.9\n",
      "Epoch: [6/7], Step: [301/3125], Training Loss: 0.8735461235046387\n",
      "Epoch: [6/7], Step: [401/3125], Validation Acc: 62.8\n",
      "Epoch: [6/7], Step: [401/3125], Training Loss: 0.7475047707557678\n",
      "Epoch: [6/7], Step: [501/3125], Validation Acc: 62.0\n",
      "Epoch: [6/7], Step: [501/3125], Training Loss: 0.9973015785217285\n",
      "Epoch: [6/7], Step: [601/3125], Validation Acc: 64.0\n",
      "Epoch: [6/7], Step: [601/3125], Training Loss: 0.7271531820297241\n",
      "Epoch: [6/7], Step: [701/3125], Validation Acc: 62.4\n",
      "Epoch: [6/7], Step: [701/3125], Training Loss: 0.7069340944290161\n",
      "Epoch: [6/7], Step: [801/3125], Validation Acc: 62.7\n",
      "Epoch: [6/7], Step: [801/3125], Training Loss: 0.7003411650657654\n",
      "Epoch: [6/7], Step: [901/3125], Validation Acc: 64.1\n",
      "Epoch: [6/7], Step: [901/3125], Training Loss: 0.7481712698936462\n",
      "Epoch: [6/7], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [6/7], Step: [1001/3125], Training Loss: 0.6316848993301392\n",
      "Epoch: [6/7], Step: [1101/3125], Validation Acc: 64.6\n",
      "Epoch: [6/7], Step: [1101/3125], Training Loss: 0.9503615498542786\n",
      "Epoch: [6/7], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [6/7], Step: [1201/3125], Training Loss: 0.65728360414505\n",
      "Epoch: [6/7], Step: [1301/3125], Validation Acc: 64.4\n",
      "Epoch: [6/7], Step: [1301/3125], Training Loss: 0.9575656056404114\n",
      "Epoch: [6/7], Step: [1401/3125], Validation Acc: 65.0\n",
      "Epoch: [6/7], Step: [1401/3125], Training Loss: 0.7385190725326538\n",
      "Epoch: [6/7], Step: [1501/3125], Validation Acc: 64.4\n",
      "Epoch: [6/7], Step: [1501/3125], Training Loss: 0.854077160358429\n",
      "Epoch: [6/7], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [6/7], Step: [1601/3125], Training Loss: 0.891764223575592\n",
      "Epoch: [6/7], Step: [1701/3125], Validation Acc: 63.1\n",
      "Epoch: [6/7], Step: [1701/3125], Training Loss: 0.6690210103988647\n",
      "Epoch: [6/7], Step: [1801/3125], Validation Acc: 62.9\n",
      "Epoch: [6/7], Step: [1801/3125], Training Loss: 0.704156756401062\n",
      "Epoch: [6/7], Step: [1901/3125], Validation Acc: 62.2\n",
      "Epoch: [6/7], Step: [1901/3125], Training Loss: 0.8584331274032593\n",
      "Epoch: [6/7], Step: [2001/3125], Validation Acc: 63.3\n",
      "Epoch: [6/7], Step: [2001/3125], Training Loss: 0.871229350566864\n",
      "Epoch: [6/7], Step: [2101/3125], Validation Acc: 63.0\n",
      "Epoch: [6/7], Step: [2101/3125], Training Loss: 0.8482323884963989\n",
      "Epoch: [6/7], Step: [2201/3125], Validation Acc: 62.3\n",
      "Epoch: [6/7], Step: [2201/3125], Training Loss: 0.7413930892944336\n",
      "Epoch: [6/7], Step: [2301/3125], Validation Acc: 63.1\n",
      "Epoch: [6/7], Step: [2301/3125], Training Loss: 0.8236175179481506\n",
      "Epoch: [6/7], Step: [2401/3125], Validation Acc: 63.3\n",
      "Epoch: [6/7], Step: [2401/3125], Training Loss: 0.8795813322067261\n",
      "Epoch: [6/7], Step: [2501/3125], Validation Acc: 63.7\n",
      "Epoch: [6/7], Step: [2501/3125], Training Loss: 0.8852208256721497\n",
      "Epoch: [6/7], Step: [2601/3125], Validation Acc: 62.8\n",
      "Epoch: [6/7], Step: [2601/3125], Training Loss: 0.7898604869842529\n",
      "Epoch: [6/7], Step: [2701/3125], Validation Acc: 65.2\n",
      "Epoch: [6/7], Step: [2701/3125], Training Loss: 0.7557382583618164\n",
      "Epoch: [6/7], Step: [2801/3125], Validation Acc: 64.2\n",
      "Epoch: [6/7], Step: [2801/3125], Training Loss: 0.7612857222557068\n",
      "Epoch: [6/7], Step: [2901/3125], Validation Acc: 63.2\n",
      "Epoch: [6/7], Step: [2901/3125], Training Loss: 0.7815359830856323\n",
      "Epoch: [6/7], Step: [3001/3125], Validation Acc: 64.0\n",
      "Epoch: [6/7], Step: [3001/3125], Training Loss: 0.8534550666809082\n",
      "Epoch: [6/7], Step: [3101/3125], Validation Acc: 64.9\n",
      "Epoch: [6/7], Step: [3101/3125], Training Loss: 0.5270694494247437\n",
      "Epoch: [7/7], Step: [101/3125], Validation Acc: 63.8\n",
      "Epoch: [7/7], Step: [101/3125], Training Loss: 0.5811880826950073\n",
      "Epoch: [7/7], Step: [201/3125], Validation Acc: 65.1\n",
      "Epoch: [7/7], Step: [201/3125], Training Loss: 0.5961688756942749\n",
      "Epoch: [7/7], Step: [301/3125], Validation Acc: 64.0\n",
      "Epoch: [7/7], Step: [301/3125], Training Loss: 0.8629978895187378\n",
      "Epoch: [7/7], Step: [401/3125], Validation Acc: 62.7\n",
      "Epoch: [7/7], Step: [401/3125], Training Loss: 0.7536587119102478\n",
      "Epoch: [7/7], Step: [501/3125], Validation Acc: 62.6\n",
      "Epoch: [7/7], Step: [501/3125], Training Loss: 1.0057886838912964\n",
      "Epoch: [7/7], Step: [601/3125], Validation Acc: 64.7\n",
      "Epoch: [7/7], Step: [601/3125], Training Loss: 0.7190839052200317\n",
      "Epoch: [7/7], Step: [701/3125], Validation Acc: 61.9\n",
      "Epoch: [7/7], Step: [701/3125], Training Loss: 0.7007777094841003\n",
      "Epoch: [7/7], Step: [801/3125], Validation Acc: 63.5\n",
      "Epoch: [7/7], Step: [801/3125], Training Loss: 0.6335729360580444\n",
      "Epoch: [7/7], Step: [901/3125], Validation Acc: 63.2\n",
      "Epoch: [7/7], Step: [901/3125], Training Loss: 0.7336022853851318\n",
      "Epoch: [7/7], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [7/7], Step: [1001/3125], Training Loss: 0.6436508297920227\n",
      "Epoch: [7/7], Step: [1101/3125], Validation Acc: 64.4\n",
      "Epoch: [7/7], Step: [1101/3125], Training Loss: 0.9357202649116516\n",
      "Epoch: [7/7], Step: [1201/3125], Validation Acc: 62.9\n",
      "Epoch: [7/7], Step: [1201/3125], Training Loss: 0.6546846032142639\n",
      "Epoch: [7/7], Step: [1301/3125], Validation Acc: 64.5\n",
      "Epoch: [7/7], Step: [1301/3125], Training Loss: 0.9131357669830322\n",
      "Epoch: [7/7], Step: [1401/3125], Validation Acc: 63.8\n",
      "Epoch: [7/7], Step: [1401/3125], Training Loss: 0.7146329879760742\n",
      "Epoch: [7/7], Step: [1501/3125], Validation Acc: 64.8\n",
      "Epoch: [7/7], Step: [1501/3125], Training Loss: 0.8315050005912781\n",
      "Epoch: [7/7], Step: [1601/3125], Validation Acc: 64.4\n",
      "Epoch: [7/7], Step: [1601/3125], Training Loss: 0.9148510694503784\n",
      "Epoch: [7/7], Step: [1701/3125], Validation Acc: 64.1\n",
      "Epoch: [7/7], Step: [1701/3125], Training Loss: 0.6614283323287964\n",
      "Epoch: [7/7], Step: [1801/3125], Validation Acc: 63.4\n",
      "Epoch: [7/7], Step: [1801/3125], Training Loss: 0.712820827960968\n",
      "Epoch: [7/7], Step: [1901/3125], Validation Acc: 63.5\n",
      "Epoch: [7/7], Step: [1901/3125], Training Loss: 0.8748660087585449\n",
      "Epoch: [7/7], Step: [2001/3125], Validation Acc: 63.2\n",
      "Epoch: [7/7], Step: [2001/3125], Training Loss: 0.893380343914032\n",
      "Epoch: [7/7], Step: [2101/3125], Validation Acc: 64.6\n",
      "Epoch: [7/7], Step: [2101/3125], Training Loss: 0.8437714576721191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/7], Step: [2201/3125], Validation Acc: 63.6\n",
      "Epoch: [7/7], Step: [2201/3125], Training Loss: 0.7245320081710815\n",
      "Epoch: [7/7], Step: [2301/3125], Validation Acc: 63.5\n",
      "Epoch: [7/7], Step: [2301/3125], Training Loss: 0.8254600167274475\n",
      "Epoch: [7/7], Step: [2401/3125], Validation Acc: 63.5\n",
      "Epoch: [7/7], Step: [2401/3125], Training Loss: 0.942622721195221\n",
      "Epoch: [7/7], Step: [2501/3125], Validation Acc: 63.5\n",
      "Epoch: [7/7], Step: [2501/3125], Training Loss: 0.9093958735466003\n",
      "Epoch: [7/7], Step: [2601/3125], Validation Acc: 63.3\n",
      "Epoch: [7/7], Step: [2601/3125], Training Loss: 0.7918506860733032\n",
      "Epoch: [7/7], Step: [2701/3125], Validation Acc: 66.0\n",
      "Epoch: [7/7], Step: [2701/3125], Training Loss: 0.7422040104866028\n",
      "Epoch: [7/7], Step: [2801/3125], Validation Acc: 64.4\n",
      "Epoch: [7/7], Step: [2801/3125], Training Loss: 0.7408944368362427\n",
      "Epoch: [7/7], Step: [2901/3125], Validation Acc: 64.7\n",
      "Epoch: [7/7], Step: [2901/3125], Training Loss: 0.775218665599823\n",
      "Epoch: [7/7], Step: [3001/3125], Validation Acc: 62.9\n",
      "Epoch: [7/7], Step: [3001/3125], Training Loss: 0.8582801818847656\n",
      "Epoch: [7/7], Step: [3101/3125], Validation Acc: 64.3\n",
      "Epoch: [7/7], Step: [3101/3125], Training Loss: 0.4490382671356201\n",
      "Max Validation Accuracy: 66.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(emb_size = 300, hidden_size=300, num_layers=1, num_classes=3).cuda()\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameters changed                  | CNN (val accuracy) | RNN (val accuracy) |\n",
    "|------------------------------------------|:------------------:|--------------------|\n",
    "| Standard                                 |        67.5%       |                    |\n",
    "| Interaction: sum                         |        62.2%       |                    |\n",
    "| Interaction: element-wise multiplication |        65.8%       |                    |\n",
    "| Regularization: Dropout (p=0.5)          |        66.6%       |                    |\n",
    "| Regularization: Dropout (p=0.2)          |        67.2%       |                    |\n",
    "| Regularization: Weight Decay (0.0001)    |        67.7%       |                    |\n",
    "| Dropout (0.2) + Weight decay (0.0001)    |        67.4%       |                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val = pd.read_csv('mnli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th percentile for sentence length (in characters): 151.0\n"
     ]
    }
   ],
   "source": [
    "sentence_length_75 = pd.Series([len(x) for x in mnli_val['sentence1']]).describe()['75%']\n",
    "print(\"75th percentile for sentence length (in characters): {}\".format(sentence_length_75))\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary of (sent1, sent2, label) data, by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_dict = {}\n",
    "for x in mnli_val['genre'].unique():\n",
    "    filtered = mnli_val[mnli_val['genre'] == x]\n",
    "    mnli_val_dict[x] = {}\n",
    "    mnli_val_dict[x][\"sent1s\"] = list(filtered[\"sentence1\"])\n",
    "    mnli_val_dict[x][\"sent2s\"] = list(filtered[\"sentence2\"])\n",
    "    mnli_val_dict[x][\"label\"] = convert_labels_to_integers(list(filtered[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "[0 1 2]\n",
      "1005\n",
      "[0 1 2]\n",
      "1002\n",
      "[0 1 2]\n",
      "1016\n",
      "[0 1 2]\n",
      "982\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "for x in mnli_val_dict.keys():\n",
    "    print(len(mnli_val_dict[x][\"sent1s\"]))\n",
    "    print(np.unique(mnli_val_dict[x][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the south , the former fishing villages of Sorrento and Positano spill down the craggy cliffs of the serpentine Amalfi coast , justifiably tauted as one of the world 's most beautiful drives .\n",
      "Sorrento used to be a fishing village .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# quick verify\n",
    "verify_order(mnli_val_dict['travel'][\"sent1s\"], mnli_val_dict['travel'][\"sent2s\"], mnli_val_dict['travel'][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed for shuffling: 51\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Good sir , Jon began .\n",
      "Jon addressed the man .\n",
      "1\n",
      "The day my deadline came , I got a business card .\n",
      "The deadline to accept my promotion arrived and I got a business card with my new title .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre fiction has validation accuracy: 46.130653266331656\n",
      "Random seed for shuffling: 29\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "really oh i thought it was great yeah\n",
      "That was horrible\n",
      "0\n",
      "i think there would be an awful lot of resentment and um i i really do n't think it would be feasible on our country\n",
      "The war would lead to a bunch of resentment among civilians .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre telephone has validation accuracy: 45.97014925373134\n",
      "Random seed for shuffling: 40\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Interesting Conflict Over Conflict of Interest\n",
      "Sometimes there is a conflict of interest .\n",
      "1\n",
      "I have a situation .\n",
      "This situation may be good or bad .\n",
      "2\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre slate has validation accuracy: 44.61077844311377\n",
      "Random seed for shuffling: 2\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Since the mid 1990s , aggregate household wealth has swelled relative to disposable personal income , largely due to increases in the market value of households ' existing assets ( see figure 1.2 ) .\n",
      "Figure 1.2 will illustrate this fact and make it easier to understand .\n",
      "2\n",
      "These aliens may seek legal assistance at any time during the year , although limited English ability and lack of knowledge of rights and procedures may provide obstacles to seeking and obtaining representation .\n",
      "These immigrants often need legal assistance .\n",
      "1\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre government has validation accuracy: 51.37795275590551\n",
      "Random seed for shuffling: 26\n",
      "\n",
      "Verifying that the data and label match after shuffling\n",
      "Be sure to look around and compare before buying .\n",
      "Make sure to browse and make comparisons before purchasing .\n",
      "1\n",
      "It spoils the sport .\n",
      "It makes it better .\n",
      "0\n",
      "\n",
      "Tokenizing sentence 1 list...\n",
      "done!\n",
      "\n",
      "Tokenizing sentence 2 list... \n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 1 list...\n",
      "done!\n",
      "\n",
      "One-hot encoding words for sentence 2 list...\n",
      "done!\n",
      "Genre travel has validation accuracy: 48.16700610997963\n"
     ]
    }
   ],
   "source": [
    "# for each genre, build validation set and evaluate on it. \n",
    "cnn_results = {}\n",
    "for genre in mnli_val_dict.keys():\n",
    "    sent1_val_indices, sent2_val_indices, val_label = data_pipeline(mnli_val_dict[genre][\"sent1s\"], \n",
    "                                                                    mnli_val_dict[genre][\"sent2s\"], \n",
    "                                                                    mnli_val_dict[genre][\"label\"])\n",
    "    val_dataset = TwoSentencesDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             collate_fn=twosentences_collate_func,\n",
    "                                             #shuffle=True\n",
    "                                             )\n",
    "    cnn_results[genre] = test_model(val_loader, model)\n",
    "    print(\"Genre {} has validation accuracy: {}\".format(genre, cnn_results[genre]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| genre      | CNN (val accuracy) | RNN (val accuracy) |\n",
    "|------------|--------------------|--------------------|\n",
    "| fiction    |       46.13%       |                    |\n",
    "| telephone  |       45.97%       |                    |\n",
    "| travel     |       48.17%       |                    |\n",
    "| slate      |       44.61%       |                    |\n",
    "| government |       51.38%       |                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Ideas\n",
    "###\n",
    "\n",
    "# Dropout layers --> prob 0.5\n",
    "# weight decay. \n",
    "\n",
    "#rnn --> layer normalize\n",
    "\n",
    "# CNN masking\n",
    "# do not backpropagate\n",
    "# after conv, cresate tensor masked not update.\n",
    "# right after regular linear layer\n",
    "# \n",
    "# set all elements until padding to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
